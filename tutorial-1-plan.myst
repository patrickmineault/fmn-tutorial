# Tutorial – building an NDT-style autoencoder from scratch

The goal of this notebook is to guide you through the *building blocks* of foundation models for neuroscience. Foundation models are trained on large-scale data in an unsupervised way, and can be adapted (fine-tuned, or steered using in-context learning) for use for a variety of downstream tasks. Training a foundation model typically involves the use of large-scale compute, but don’t let that scare you away! We can cover core building blocks, like transformers, tokenization, training and fine-tuning in a short tutorial. 

We will train an NDT-1-stitch style model from scratch. NDT-1 (neural data transformer-1) is a model that is trained to predict missing spike data via a masked auto-encoder. It takes in spike data and recovers spike rates. The base NDT-1 is specific to a session: it’s not a foundation model. It’s conceptually similar to models like LFADS or GPFA that attempt to find latents from spike data.

However, a simple extension (NDT-1-stitch) can stitch sessions from different subjects and experiments together. That allows it to be trained at large scale as a foundation model. While there are now many more effective foundation models for neuroscience that can be used for spike data (e.g. POYO), NDT-1-stitch covers most of the relevant ingredients to build a successful foundation model:

* Building transformers that can read and embed spike data
* Creating a self-supervised pretraining task that learns good representations of neural data
* Training a model
* Tracking the performance of a model during self-supervised pretraining
* Building embedding and readout mechanisms that can be used to adapt to different sessions and brains
* Fine-tuning a model trained on one task to another task

Let’s train an NDT-1-stitch style model from scratch. 

## A toy task: the Lorenz dataset

We’ll start by applying our model to artificially generated data. Because the data is artificially generated, we know exactly what the latents are, and it will be easy to validate that our model is working correctly. The Lorenz dataset is a toy, artificial dataset generated by the latents of the Lorenz attractor. The [Lorenz attractor](https://en.wikipedia.org/wiki/Lorenz_system) is a 3-variable chaotic system that unfolds over time. The Lorenz dataset is generated by taking projections of these variables, scaling them to obtain spike rates, and generating spikes via a Poisson process. In other words:

$$
\begin{align}
\mathbf{x}_{t+1} = \text{Lorenz}(\mathbf{x}(t)) \\
\boldsymbol{\lambda}(t) &= \exp(\mathbf{C}\mathbf{x}(t) + \mathbf{d}) \\
\mathbf{y}(t) &\sim \text{Poisson}(\boldsymbol{\lambda}(t))
\end{align}
$$

The Lorenz dataset has become something of a standard for debugging models that can infer latents from observations, since it was first used to benchmark the LFADS model. Let's look at some of the data:

```{code-cell}
def load_dataset(name):
    with open(f"../data/{name}_data.pkl", "rb" as f):
        return pickle.load(f)

data = load_dataset("lfads_lorenz")
print(data.keys())
```

These are the important keys:

* `train_data`: training data, an array of shape `(n_trials, n_timepoints, n_neurons)`. They correspond to the number of spikes (in simulated 10 ms bins) for each trial, for each neuron
* `val_data` and `val_behavior`: same, but for a validation fold. We've pre-split the data into train and validation folds. We'll train on the train fold and validate our model on the validation fold.
* `val_truth`: ground truth data for the true underlying spike rates. We know the ground truth because this is an artificial dataset.

Now let's look at some data:

```{code-cell}
import matplotlib.pyplot as plot

bin_size = 0.01 # in seconds

# One figure, two axes stacked vertically
fig, axes = plt.subplots(
    nrows=1, ncols=2,           # two rows, one column
    figsize=(6, 8),             # any size you like
    sharex=False                 # optional: share the x-axis
)

axes[0].imshow(dataset['val_data'][0, :, :].T, cmap='gray_r', aspect='auto', extent=[0, dataset['val_data'].shape[1] * bin_size, 0, dataset['train_data'].shape[2]])
axes[0].set_xlabel('Time (s)')
axes[0].set_ylabel('Neuron #')
axes[0].set_title('Spike rates')

axes[1].imshow(dataset['val_ground_truth'][0, :, :].T, cmap='gray_r', aspect='auto', extent=[0, dataset['val_data'].shape[1] * bin_size, 0, dataset['train_data'].shape[2]])
axes[1].set_xlabel('Time (s)')
axes[1].set_ylabel('Neuron #')
axes[1].set_title('Spike rates')
```

To make sure we’re doing well, we’ll use another dataset where we know what the actual latents are. 


## The datasets and the task

Let’s visualize what we’re working with. A typical experiment where you might want to apply a foundation model for neuroscience is a BCI decoding experiment. In the `mc_maze` series of datasets (`mc_maze`, `mc_maze_large`, `mc_maze_medium`, `mc_maze_small`), a monkey completes a reaching task where he needs to trace with his finger on a touchscreen from a start position to an end position, avoiding the maze walls. Neurons are recorded in premotor cortex (PMd) and in M1. 

I've created pickle files from the raw data, which is stored in the DANDI archive. The purpose of these pickle files is to make it easy for you to read this raw data and train a model from it. Let's see the different keys in the pickle file:

```
import pickle

with open("mc_maze_data.pkl", "rb") as f:
    data = pickle.load(f)

pickle.keys()
```

These are the important keys:

```
* `train_data`: training data, an array of shape `(n_trials, n_timepoints, n_neurons)`. They correspond to the number of spikes (in 10 ms bins) for each trial, for each neuron
* `train_behavior`: Aligned behavior, an array of shape `(n_trials, 2)`. This corresponds to the monkey's arm velocity, in `m/s`
* `val_data` and `val_behavior`: same, but for a validation fold. We've pre-split the data into train and validation folds. We'll train on the train fold and validate our model on the validation fold.
* `val_ground_truth`: "Ground truth" data for the true underlying spike rates. This was generated by taking an average over similar trials and applying a 50 ms smoothing window. 

Let’s see some raster plots from one example trial. 

[code]

Notice that the data is quite noisy. There are many things we can do with this dataset, including:

- Recovering the underlying spike rates of the data from spikes. Although data 
- Predicting the monkey’s arm position. Perhaps we’d like to train a BCI. That would involve decoding the monkey’s hand position from just his brain activity. We could train based on the monkey’s real movement, and then apply the decoder so the monkey can simply imagine the movement instead.


## Section 1: recovering the latents via a single-session autoencoder

[show the image of NDT-1 here]

1. Our goal is to find a good representation of this data. We’ll create a masked autoencoder. We’ll use transformers. Transformers work on tokens, so we need to decide on a tokenization scheme. NDT-1 makes a simple choice: one time bin, all neurons = one token. Note that there are many other tokenization schemes:
    1. one token = multiple time bins, multiple neurons (patching)
    2. one token = all time bins, one neuron
    3. one token = one spike (the POYO choice)
2. Next, we need a pretext task to encourage our model to learn a good representation of the data. Masked autoencoding is such a task. 
    1. Split tokens into two: masked (25%) and unmasked (75%)
    2. For the masked tokens, 80% will be replaced with zeros, 5% will be replaced by noise, the rest will be left alone
    3. The unmasked tokens are untouched
    4. The task: predict unmasked tokens from all (masked + unmasked tokens)
    5. (demonstrate this in a cartoon fashion)
3. With that, we have our core ingredients. Let’s build a simple transformer that takes in the raw data and predicts the left-out token
    1. The latent dimensionality is the same as the dimensionality of the data
    2. We add some positional encodings over time
    3. We add an N-layer transformer (we use N=2)
    4. At the end of the line, we have an output nonlinearity
    5. We add a penalty for the log-likehood (Poisson loss)
    6. We train the model (train within the colab)
4. Let’s verify this finds some reasonable latents (look at the true latents and see whether they align with the inferred ones)
5 (bonus). What do we know about the role of different position embeddings? Watch what happens if we remove the sinusoid embeddings
6 (bonus). Consider a next-token prediction task instead

## Section 2: an autoencoder that can switch different recordings

Thus far, we’ve trained an auto-encoder from scratch. However, it is very specific to this recording session: what if we had recordings from two different monkeys doing the same task? Then we’d need to train two separate autoencoders. There is no natural link between the units for monkey 1 and for monkey 2.

In this section, we’ll repeat the same exercise but *in latent space*. We use a scheme inspired by NDT-1-stitch, which is also similar to the one used by many others, including POYO: we use a linear projection to perform the analysis in latent space. The latent space does alignment.

<show image of NDT-1 stitch>

- Add the linear encoder and decoder layers
- Apply on the Lorenz dataset–what do the embeddings look like? Look at the first two PCs of the embeddings and see what they look like
- Apply on the mc-maze small dataset → can we read off the identity of the relevant areas?
- Apply on mc-maze small and medium datasets → do the embeddings look similar? What would happen if swap in one embedding for the other?

## Section 3: using the model

So far, we’ve used a pretext task to learn a good representation of neural data. But how can we use this model to perform a downstream task? Let’s use one downstream task, decoding the hand velocity of a monkey during a task. This could be the basis for a brain-computer interface, where we replace the monkey’s real movement with an intended movement of a cursor.

- Frozen model, linear decoder: let’s put a linear decoder on top of the model. We strip out the top of the model that transforms the latents into spike rates, and instead put a linear regression on top of it. There’s necessarily a lag between the monkey’s brain activity and the movement of the arm.
    - Using a grid search on the latents of the model, let’s find the best lag. What is it? Let’s use this lag from now on.
    - Let’s evaluate the model in a leave-one-out environment. How well does this work?
- Frozen model, causal linear decoder. One issue here is that our model is acausal: although the linear decoder uses only past data, the transformer mixes tokens from the past *and* the future. That means we could not use the model as is as an online decoder. Let’s use attention masking to ensure that the model only uses past spike data. How well does that work?
- Causal linear decoder, end-to-end training. It’s not clear that the pretrained model will work well, because the pretext task could leverage the past and future. Can we do better if we train the model end-to-end?
- Did we actually do anything? Train a very simple causal lagged linear regression model end-to-end. How well does that work?

## Section 4: scaling up

We’ve trained our model on a tiny amount of data. We could scale up to vastly larger amounts of data with a significant amount of engineering. A big question in building foundation models for neuroscience is dealing with the nuts and bolts of data engineering: neural data can be in trials, continuous, readouts can be discrete, continuous, etc. And that means that to scale up to thousands of hours of data, we need to work on good data pipelines. See this tutorial from Eva Dyer’s group from Cosyne here:

- https://cosyne-tutorial-2025.github.io/
