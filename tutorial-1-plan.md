# Tutorial 1

The goal of this notebook is to guide you through the *building blocks* of foundation models for neuroscience. Foundation models are trained on large-scale data in an unsupervised way, and can be adapted (fine-tuned, or steered using in-context learning) for use for a variety of downstream tasks. Training a foundation model typically involves the use of large-scale compute, but don’t let that scare you away! We can cover core building blocks, like transformers, tokenization, training and fine-tuning in a short tutorial. 

We will train an NDT-1-stitch style model from scratch. NDT-1 (neural data transformer-1) is a model that is trained to predict missing spike data via a masked auto-encoder. It takes in spike data and recovers spike rates. The base NDT-1 is specific to a session: it’s not a foundation model. It’s conceptually similar to models like LFADS or GPFA that attempt to find latents from spike data.

However, a simple extension (NDT-1-stitch) can stitch sessions from different subjects and experiments together. That allows it to be trained at large scale as a foundation model. 

Let’s train an NDT-1-stitch style model from scratch. 

## The datasets and the task

Let’s visualize what we’re working with. A typical experiment where you might want to apply a foundation model for neuroscience is a BCI decoding experiment. In the `mc_maze` dataset a monkey completes a reaching task where he needs to trace with his finger on a touchscreen from a start position to an end position, avoiding the maze walls. Neurons are recorded in pMd and in M1. 

Let’s see the raster plots from sample data. 

[code]

There are many things we can do with this dataset, including:

- Recovering the latents. Although many neurons are recorded, the data live in a low-dimensional space. What do those latents look like?
- Predicting the monkey’s arm position. Perhaps we’d like to train a BCI. That would involve decoding the monkey’s hand position from just his brain activity. We could train based on the monkey’s real movement, and then apply the decoder so the monkey can simply imagine the movement instead.

We’ll start by recovering latents. To make sure we’re doing well, we’ll use another dataset where we know what the actual latents are. The Lorenz dataset: a toy, artificial dataset generated by the latents of the Lorenz attractor. The Lorenz attractor is a 3-variable chaotic system that unfolds over time. The Lorenz dataset is generated by taking projections of these variables, scaling them to obtain spike rates, and generating spikes via a Poisson process.

## Section 1: recovering the latents via a single-session autoencoder

[show the image of NDT-1 here]

1. Our goal is to find a good representation of this data. We’ll create a masked autoencoder. We’ll use transformers. Transformers work on tokens, so we need to decide on a tokenization scheme. NDT-1 makes a simple choice: one time bin, all neurons = one token. Note that there are many other tokenization schemes:
    1. one token = multiple time bins, multiple neurons (patching)
    2. one token = all time bins, one neuron
    3. one token = one spike (the POYO choice)
2. Next, we need a pretext task to encourage our model to learn a good representation of the data. Masked autoencoding is such a task. 
    1. Scheme: replace 20% of tokens (= time points) with zeros. Use the rest of the data to predict the left-out data.
    2. The masked out tokens are filled with zeros and processed from scratch
3. With that, we have our core ingredients. Let’s build a simple transformer that takes in the raw data and predicts the left-out token
    1. The latent dimensionality is the same as the dimensionality of the data
    2. We add some positional encodings over time
    3. We add an N-layer transformer (we use N=2)
    4. At the end of the line, we have an output nonlinearity
    5. We add a penalty for the log-likehood (Poisson loss)
    6. We train the model (train within the colab)
4. Let’s verify this finds some reasonable latents (look at the true latents and see whether they align with the inferred ones)

## Section 2: an autoencoder that can switch different recordings

Thus far, we’ve trained an auto-encoder from scratch. However, it is very specific to this recording session: what if we had recordings from two different monkeys doing the same task? Then we’d need to train two separate autoencoders. There is no natural link between the units for monkey 1 and for monkey 2.

In this section, we’ll repeat the same exercise but *in latent space*. We use a scheme inspired by NDT-1-stitch, which is also similar to the one used by many others, including POYO: we use a linear projection to perform the analysis in latent space. The latent space does alignment.

<show image of NDT-1 stitch>

- Add the linear encoder and decoder layers
- Apply on the Lorenz dataset–what do the embeddings look like? Look at the first two PCs of the embeddings and see what they look like
- Apply on the mc-maze small dataset → can we read off the identity of the relevant areas?
- Apply on mc-maze small and medium datasets → do the embeddings look similar? What would happen if swap in one embedding for the other?

## Section 3: using the model

So far, we’ve used a pretext task to learn a good representation of neural data. But how can we use this model to perform a downstream task? Let’s use one downstream task, decoding the hand velocity of a monkey during a task. This could be the basis for a brain-computer interface, where we replace the monkey’s real movement with an intended movement of a cursor.

- Frozen model, linear decoder: let’s put a linear decoder on top of the model. We strip out the top of the model that transforms the latents into spike rates, and instead put a linear regression on top of it. There’s necessarily a lag between the monkey’s brain activity and the movement of the arm.
    - Using a grid search on the latents of the model, let’s find the best lag. What is it? Let’s use this lag from now on.
    - Let’s evaluate the model in a leave-one-out environment. How well does this work?
- Frozen model, causal linear decoder. One issue here is that our model is acausal: although the linear decoder uses only past data, the transformer mixes tokens from the past *and* the future. That means we could not use the model as is as an online decoder. Let’s use attention masking to ensure that the model only uses past spike data. How well does that work?
- Causal linear decoder, end-to-end training. It’s not clear that the pretrained model will work well, because the pretext task could leverage the past and future. Can we do better if we train the model end-to-end?
- Did we actually do anything? Train a very simple causal lagged linear regression model end-to-end. How well does that work?

## Section 4: scaling up

We’ve trained our model on a tiny amount of data. We could scale up to vastly larger amounts of data with a significant amount of engineering. A big question in building foundation models for neuroscience is dealing with the nuts and bolts of data engineering: neural data can be in trials, continuous, readouts can be discrete, continuous, etc. And that means that to scale up to thousands of hours of data, we need to work on good data pipelines. See this tutorial from Eva Dyer’s group from Cosyne here:

- https://cosyne-tutorial-2025.github.io/
