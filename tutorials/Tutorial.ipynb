{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1760a3f9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Tutorial: Foundations of foundation models for neuroscience\n",
    "\n",
    "The goal of this notebook is to guide you through the *building blocks* of foundation models for neuroscience. Foundation models are trained on large-scale data in an unsupervised way, and can be adapted (fine-tuned, or steered using in-context learning) for use for a variety of downstream tasks. Training a foundation model typically involves the use of large-scale compute, but don't let that scare you away! We will cover core building blocks, like transformers, tokenization, training and fine-tuning in this tutorial. \n",
    "\n",
    "## Our objective: building up to NDT-1-stitch\n",
    "\n",
    "We will train an NDT-1-stitch style model from scratch. NDT-1 (neural data transformer-1, [Ye and Pandarinath 2021](https://arxiv.org/abs/2108.01210)) is a model that is trained to predict missing spike data via a masked auto-encoder. \n",
    "\n",
    "![NDT-1 architecture](images/ndt_model.png)\n",
    "\n",
    "It takes in spike data and recovers spike rates. \n",
    "\n",
    "![NDT-1 action](image/ndt_action.png)\n",
    "\n",
    "At the heart of NDT-1 is a transformer that reads in spike data, and outputs a denoised version of the spike data. \n",
    "\n",
    "![NDT-1 transformer](images/ndt_transformer.png)\n",
    "\n",
    "NDT-1 is a self-supervised model that learns representations through a masked auto-encoder task. It's inspired by the masked auto-encoder (MAE) pre-training task that's used in BERT. Consider the following sentence:\n",
    "\n",
    "```\n",
    "The cat sat on the [MASK].\n",
    "                      |\n",
    "                     mat\n",
    "```\n",
    "\n",
    "A large-language model like BERT is trained, on millions of sentences, to predict masked words like `[MASK]` from the surrounding context. The model learns to predict the masked word, and in doing so, learns a representation of the sentence that captures the meaning of the words in context. NDT-1 uses an adaptation of the same masked autoencoding task, but with spike data. \n",
    "\n",
    "## From NDT-1 to NDT-1-stitch\n",
    "\n",
    "The base NDT-1 works across a single session, with a fixed set of neurons. A number of extensions have been proposed that allows one to stitch together multiple sessions, and even multiple subjects, into a single model. This ultimately allows models to be trained on large-scale heterogeneous datasets, and is a key enabler of a foundation model for neuroscience. \n",
    "\n",
    "Our implementation is inspired by NDT1-stitch, which has been shown to be effective at scaling beyond a single session ([Ye et al. 2023](https://www.biorxiv.org/content/10.1101/2023.09.18.558113v1), [Zhang et al. 2024](https://arxiv.org/abs/2407.14668))\n",
    "\n",
    "While there are now many newer, and more effective foundation models for neuroscience that can be used for spike data (e.g. POYO, POSSM, etc.), NDT-1-stitch covers most of the relevant ingredients to build a successful foundation model.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "We'll cover the following topics in this tutorial:\n",
    "\n",
    "* Building transformers that can read and embed spike data\n",
    "* Creating a self-supervised pretraining task that learns good representations of neural data\n",
    "* Training a model\n",
    "* Tracking the performance of a model during self-supervised pretraining\n",
    "* Building embedding and readout mechanisms that can be used to adapt to different sessions and brains\n",
    "* Fine-tuning a model trained on one task to another task\n",
    "\n",
    "By the end of this tutorial, you will have a good understanding of the core building blocks of foundation models for neuroscience, and how to implement them in PyTorch. You'll be in great shape to read the literature and identify how models are built and evaluated. \n",
    "\n",
    "What we won't cover in this tutorial is *how* to scale up to massive compute, or how to train models on large-scale datasets. We have references at the end that cover these topics.\n",
    "\n",
    "With that out of the way, let's train an NDT-1-stitch style model from scratch!\n",
    "\n",
    "## A toy task: the Lorenz dataset\n",
    "\n",
    "We'll start by applying our model to artificially generated data. Because the data is artificially generated, we know exactly what the latents are, and it will be easy to validate that our model is working correctly. The Lorenz dataset is a toy, artificial dataset generated by the latents of the Lorenz attractor. The [Lorenz attractor](https://en.wikipedia.org/wiki/Lorenz_system) is a 3-variable chaotic system that unfolds over time. The Lorenz *dataset* is generated by taking projections of these variables, scaling them to obtain spike rates, and generating spikes via a Poisson process. In other words:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{t+1} &= \\text{Lorenz}(\\mathbf{x}(t)) \\\\\n",
    "\\boldsymbol{\\lambda}(t) &= \\exp(\\mathbf{C}\\mathbf{x}(t) + \\mathbf{d}) \\\\\n",
    "\\mathbf{y}(t) &\\sim \\text{Poisson}(\\boldsymbol{\\lambda}(t))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The Lorenz dataset has become something of a standard for debugging models that can infer latents from observations, as it was used to benchmark the LFADS model. Let's have a look at the data to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a74eb",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Import necessary libraries\n",
    "import argparse\n",
    "import math\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU available. Training will be slow.\")\n",
    "    print(\"If you are running this in Google Colab, make sure to enable GPU acceleration in the Runtime settings.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde6c04",
   "metadata": {
    "cellView": "form",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#@title Load data and tools\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/patrickmineault/fmn-tutorial.git\n",
    "    !pip install -q -r -e fmn-tutorial/\n",
    "    !mv fmn-tutorial/data data\n",
    "    !mv fmn-tutorial/checkpoints checkpoints\n",
    "else:\n",
    "    !cp -r ../data data\n",
    "    !cp -r ../checkpoints checkpoints\n",
    "    print(\"Running locally - skipping git clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48e90e",
   "metadata": {
    "cellView": "form",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#@title Visualization functions\n",
    "def plot_trial_data(trial_data, trial_truth, bin_size=0.01):\n",
    "    \"\"\"\n",
    "    Helper function to plot the trial data and truth.\"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2,\n",
    "        ncols=1,  # two rows, one column\n",
    "        figsize=(4, 6),  # any size you like\n",
    "        sharex=False,  # optional: share the x-axis\n",
    "    )\n",
    "\n",
    "    axes[0].imshow(\n",
    "        trial_data.T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            trial_data.shape[0] * bin_size,\n",
    "            0,\n",
    "            trial_data.shape[1],\n",
    "        ],\n",
    "    )\n",
    "    axes[0].set_ylabel(\"Neuron #\")\n",
    "    axes[0].set_title(\"Spikes\")\n",
    "\n",
    "    axes[1].imshow(\n",
    "        dataset[\"val_truth\"][0, :, :].T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            trial_truth.shape[0] * bin_size,\n",
    "            0,\n",
    "            trial_truth.shape[1],\n",
    "        ],\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Time (s)\")\n",
    "    axes[1].set_ylabel(\"Neuron #\")\n",
    "    axes[1].set_title(\"Spike rates\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def visualize_masking(data, masked_data, mask):\n",
    "    \"\"\"Visualize the masking function.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "    # Create 4-panel visualization\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 8))\n",
    "\n",
    "    # Since we have a single sequence, we'll plot as line plots for clarity\n",
    "    time_axis = np.arange(original_data.shape[1])\n",
    "\n",
    "    axes[0].imshow(\n",
    "        original_data.squeeze().numpy().T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            data.shape[1] * bin_size,\n",
    "            0,\n",
    "            data.shape[2],\n",
    "        ],\n",
    "    )\n",
    "    axes[0].set_title(\"Original Data (Non-masked)\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[0].set_xlabel(\"Time Steps\")\n",
    "    axes[0].set_ylabel(\"Spike Value\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].imshow(torch.tile(mask, [n_neurons, 1]).squeeze(), cmap=\"gray\")\n",
    "    axes[1].set_title(\"Mask (white = masked positions)\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[1].set_xlabel(\"Time Steps\")\n",
    "    axes[1].set_ylabel(\"Mask Value\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].imshow(\n",
    "        masked_data.squeeze().T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            data.shape[1] * bin_size,\n",
    "            0,\n",
    "            data.shape[2],\n",
    "        ],\n",
    "    )\n",
    "    axes[2].set_title(\"Masked Data\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[2].set_xlabel(\"Time Steps\")\n",
    "    axes[2].set_ylabel(\"Spike Value\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91677792",
   "metadata": {
    "cellView": "form",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#@title Utility functions\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\"Linear warmup and then cosine decay.\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
    "    Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
    "    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=0.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(\n",
    "            optimizer, self.lr_lambda, last_epoch=last_epoch\n",
    "        )\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(\n",
    "            max(1, self.t_total - self.warmup_steps)\n",
    "        )\n",
    "        return max(\n",
    "            0.0, 0.5 * (1.0 + math.cos(math.pi * float(self.cycles) * 2.0 * progress))\n",
    "        )\n",
    "\n",
    "def calculate_pseudo_r2(A, B):\n",
    "    \"\"\"Calculate the average pseudo R² between two tensors A and B.\n",
    "\n",
    "    A and B are two matrices of shape (n_samples, n_dimensions).\n",
    "\n",
    "    By pseudo R² we mean the square of the correlation coefficient comparing A and B.\n",
    "\n",
    "    We use this rather than R^2 because it's insensitive to scaling.\n",
    "\n",
    "    Hence, r2 = 1 / n_dimensions sum_i corrcoef(A[:, i], B[:, i]) ^ 2\n",
    "    \"\"\"\n",
    "    assert A.shape == B.shape, \"A and B must have the same shape\"\n",
    "    assert A.ndim == 2, \"A and B must be 2D matrices\"\n",
    "    corr_mat = torch.corrcoef(torch.concat([A, B], dim=1).T)\n",
    "    corrs = torch.diag(corr_mat[: corr_mat.shape[0] // 2, corr_mat.shape[0] // 2 :])\n",
    "    return (corrs**2).mean().item()\n",
    "\n",
    "bin_size = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160c00c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def load_dataset(name):\n",
    "    with open(f\"data/{name}_data.pkl\", \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "dataset = load_dataset(\"lfads_lorenz\")\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391c36c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "These are the important keys:\n",
    "\n",
    "* `train_data`: training data, an array of shape `(n_trials, n_timepoints, n_neurons)`. They correspond to the number of spikes (in simulated 10 ms bins) for each trial, for each neuron\n",
    "* `val_data` and `val_behavior`: same, but for a validation fold. We've pre-split the data into train and validation folds. We'll train on the train fold and validate our model on the validation fold.\n",
    "* `val_truth`: ground truth data for the true underlying spike rates. We know the ground truth because this is an artificial dataset.\n",
    "\n",
    "Now let's look at the data for a single trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trial_data(dataset[\"val_data\"][0], dataset[\"val_truth\"][0], bin_size = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c09c75",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Our job will be to learn a model that can take spike data, like the one at the top, and give us back rate data, like what we have at the bottom. Importantly, we can't train the model to derive the spike rate data directly, because in general we won't have access to that ground truth! We need to be a bit more clever, using this to build an auto-encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440be504",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Building a transformer auto-encoder\n",
    "\n",
    "Our first order of business is to create a model that can take in spike data and return (denoised) spike data: an auto-encoder. This is going to be the scheme for our auto-encoder:\n",
    "\n",
    "* We take one trial worth of spike data (`n_timepoints`, `n_neurons`) and embed into a series of tokens (`n_tokens`, `latent_dim`).\n",
    "* We pass these tokens through a series of transformer layers. These transformer layers mix information across timepoints neurons, and return a new set of tokens (n_tokens, latent_dim)\n",
    "* At the end, we then decode back into spike data (`n_timepoints`, `n_neurons`)\n",
    "\n",
    "## What's a transformer?\n",
    "\n",
    "We assume that you've encountered transformers before, but if you haven't, here's a quick refresher. Transformers are a type of neural network architecture that was introduced in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762). They are designed to process sequences of data, and they do so by using self-attention mechanisms to weigh the importance of different parts of the input sequence.\n",
    "\n",
    "We will not get into the math of transformers here (you should have encountered them in earlier tutorials), but we will use them to process our spike data. The key idea is that transformers can learn to represent sequences of data in a way that captures the relationships between different parts of the sequence. This is done by using self-attention mechanisms that allow the model to focus on different parts of the input sequence when making predictions.\n",
    "\n",
    "## What's a token?\n",
    "\n",
    "A token in the context of transformers is the fundamental unit of information that the model processes. It as a discrete, fixed-size representation that captures some meaningful aspect of your input data: a vector. \n",
    "\n",
    "When working with text, words or subwords are often used as tokens. When working with images, patches of pixels can be used as tokens. And so on and so forth.\n",
    "\n",
    "The big question is: how do we turn spike data into tokens?\n",
    "\n",
    "We'll start with the simplest possible scheme: one token corresponds to all the spike data from a single time bin. That means we take the spike data, and for each time bin, we create a token that contains the spike counts for all neurons at that time bin.\n",
    "\n",
    "In this scheme, `n_tokens` = `n_timepoints`, and `latent_dim` = `n_neurons`. \n",
    "\n",
    "Let's write out the corresponding network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19250f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 1,\n",
    "        ffn_dim: int = 256,\n",
    "        dropout: float = 0.5,\n",
    "        max_seq_len: int = 1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(max_seq_len, input_dim))\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        def create_encoder_layer() -> nn.TransformerEncoderLayer:\n",
    "            return nn.TransformerEncoderLayer(\n",
    "                d_model=self.input_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ffn_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                norm_first=True,\n",
    "            )\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [create_encoder_layer() for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # This projects the output back to the input dimension\n",
    "        self.output_projection = nn.Linear(input_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer autoencoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed tensor of shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "\n",
    "        # Project input and add positional encoding\n",
    "        x = math.sqrt(self.input_dim) * x + self.pos_embedding[:seq_len, :].unsqueeze(0)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through transformer encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.dropout(self.norm(x))  # Final layer normalization\n",
    "        x = self.output_projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "n_neurons = 29  # Number of neurons in the Lorenz dataset\n",
    "net = SimpleTransformerAutoencoder(n_neurons)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8180869e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Understanding the positional encoding\n",
    "\n",
    "Aside from the all-important transformer encoder layers, this model has a positional encoding layer that adds positional information to the input tokens. This is important because transformers do not have any inherent notion of order, and we need to provide that information explicitly.\n",
    "\n",
    "The positional encoding is initialized in the __init__ constructor:\n",
    "\n",
    "```\n",
    "self.pos_embedding = nn.Parameter(torch.zeros(max_seq_len, input_dim))\n",
    "```\n",
    "\n",
    "It's directly added to the input tokens in the forward pass:\n",
    "\n",
    "```\n",
    "x = math.sqrt(self.input_dim) * x + self.pos_embedding[:seq_len, :].unsqueeze(0)\n",
    "```\n",
    "\n",
    "First, we scale the input; then, we add the positional encoding. We use unsqueeze(0) to add a batch dimension, since all trials in the batch share the same positional embedding.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Another important point is that we use dropout at multiple points in the network. This is a form of regularization that helps prevent overfitting. Dropout randomly sets some of the activations to zero during training, which forces the model to learn more robust features. The NDT-1 paper notes that aggressive dropout is **critical** to make the network learn meaningful features.\n",
    "\n",
    "## Defining the pretext task\n",
    "\n",
    "Our next order of business is to define a pretext task that will encourage the model to learn a good representation of the data. The pretext task we will use is a masked autoencoder. The idea is to randomly mask out some of the tokens in the input sequence, and then train the model to predict the masked tokens from the unmasked ones.\n",
    "\n",
    "Let's see what the masking function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e542d5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def do_masking(\n",
    "    batch: torch.Tensor, mask_ratio: float = 0.25\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Randomly mask *mask_ratio* timesteps per trial (span width = 1).\"\"\"\n",
    "    batch_size, num_timesteps = batch.shape[:2]\n",
    "    mask = torch.rand(batch_size, num_timesteps) < mask_ratio\n",
    "\n",
    "    # Replace some masked tokens with 0 (80%) or random spikes (20%)\n",
    "    mask_token_ratio = 0.8\n",
    "    random_token_ratio = 0.25\n",
    "\n",
    "    replace_zero = (torch.rand_like(mask, dtype=float) < mask_token_ratio) & mask\n",
    "    replace_rand = (\n",
    "        (torch.rand_like(mask, dtype=float) < random_token_ratio) & mask & ~replace_zero\n",
    "    )\n",
    "\n",
    "    batch_mean = batch.float().mean().item()\n",
    "    batch = batch.clone()  # avoid in‑place modification\n",
    "    batch[replace_zero] = 0\n",
    "    if replace_rand.any():\n",
    "        rand_values = (torch.rand_like(batch, dtype=float) < batch_mean).to(torch.int)\n",
    "        batch[replace_rand] = rand_values[replace_rand]\n",
    "\n",
    "    return batch, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8757c1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "It's a little complicated, so let's break this down:\n",
    "\n",
    "* The masking function takes in a batch of spike data and a mask ratio (default 0.25).\n",
    "* It randomly masks out some of the tokens in the batch, with a span width of 1 (i.e., each token is masked independently).\n",
    "* For each masked token:\n",
    "    * with 80% probability, it's replaced with zeros\n",
    "    * if it's not replaced by zeros, then with 25% probability, it is replaced with random spikes (that is, 25% of 20% are replaced with random spikes, or 5% of the total masked tokens).\n",
    "    * the remaining 15% of the masked tokens are left unchanged.\n",
    "* The unmasked tokens are untouched.\n",
    "\n",
    "This complex recipe mirrors the one used in the original BERT paper, and is designed to encourage the model to learn a good representation of the data by predicting the masked tokens from the unmasked ones. The three different replacement strategies (zero, random, and unchanged) encourage the model to learn different aspects of the data:\n",
    "\n",
    "- The zero replacement encourages the model to learn to predict the missing tokens from the context.\n",
    "- The random replacement encourages the model to learn to ignore noise in the data.\n",
    "- The unchanged tokens encourage the model to learn to predict the unmasked tokens from the context.\n",
    "\n",
    "Let's see what happens when we apply this masking function to fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(48)  # For reproducible results\n",
    "original_data = torch.tensor(\n",
    "    dataset[\"train_data\"][0:1, :, :]\n",
    ")  # Use the first trial from the training data\n",
    "\n",
    "masked_data, mask = do_masking(original_data, 0.25)\n",
    "visualize_masking(original_data, masked_data, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034c3dc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice, in particular, how the masked data has many more zeros at early time steps. The challenge is then to reconstruct the original data from the masked data.\n",
    "\n",
    "## The loss function\n",
    "\n",
    "Once we have the masked data, we need to define a loss function that will encourage the model to learn to predict the masked tokens. The loss function we will use is the Poisson negative log-likelihood loss, which is appropriate for spike data. This loss function measures how well the model predicts the spike counts in the masked tokens, given the unmasked tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.PoissonNLLLoss(reduction=\"none\", log_input=True)\n",
    "# For one example batch, let's see how this works:\n",
    "# Use the first trial from the training data\n",
    "spikes = torch.tensor(dataset[\"train_data\"][0:16, :, :]).to(torch.int)\n",
    "mask_ratio = 0.25  # Masking ratio for the pretext task\n",
    "spikes_masked, mask = do_masking(spikes, mask_ratio)\n",
    "preds = net(spikes_masked.float())\n",
    "loss = criterion(preds[mask], spikes[mask]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c0d30",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice that:\n",
    "\n",
    "* We only use the masked tokens to compute the loss\n",
    "* We use the Poisson negative log-likelihood loss, which is appropriate for spike data\n",
    "\n",
    "## Augmentation\n",
    "\n",
    "It's coming to augment data in deep learning pipelines: for example, we might include random shifts and zooms in images to make the model more robust to small perturbations. In the case of spike data, we can use a similar idea: we can randomly shift the time series data by a small amount, and then train the model to predict the shifted data. We'll do the shifting in the collate function, which is called when we create batches of data. This will allow us to shift the data randomly for each batch, and make the model more robust to small shifts in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20562f3e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def circshift_collate_fn(batch, delta=1):\n",
    "    def fun(batch):\n",
    "        \"\"\"Vectorized version for better performance\"\"\"\n",
    "        data_list, truth_list = zip(*batch)\n",
    "\n",
    "        data = torch.stack(data_list)\n",
    "        truth = torch.stack(truth_list)\n",
    "\n",
    "        batch_size = data.size(0)\n",
    "        seq_len = data.size(1)\n",
    "\n",
    "        # Generate random shifts for each sample\n",
    "        shifts = torch.randint(-delta, delta + 1, (batch_size,))\n",
    "\n",
    "        # Apply shifts using advanced indexing\n",
    "        indices = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n",
    "        shifted_indices = (indices - shifts.unsqueeze(1)) % seq_len\n",
    "\n",
    "        # Apply the shifts\n",
    "        data = data.gather(\n",
    "            1, shifted_indices.unsqueeze(-1).expand(-1, -1, data.size(-1))\n",
    "        )\n",
    "        return data, truth\n",
    "\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956f6fc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Putting it all together and training the model\n",
    "\n",
    "With that, we're ready to learn a model that can take in the masked data and predict the unmasked tokens. We're going to create our basic training loop, which should look familiar by now:\n",
    "\n",
    "* Load the data (including the circshift augmentation)\n",
    "* Create the model\n",
    "* Create the loss function\n",
    "* Create the optimizer\n",
    "* For each epoch:\n",
    "    * For each batch:\n",
    "        * Mask the data\n",
    "        * Pass the masked data through the model\n",
    "        * Compute the loss\n",
    "        * Backpropagate and update the model parameters\n",
    "    * Every few iterations:\n",
    "        * Calculate the validation loss\n",
    "\n",
    "Let's put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    net: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    mask_ratio: float,\n",
    ") -> float:\n",
    "    net.train()\n",
    "    epoch_losses = []\n",
    "    for spikes, _ in loader:  # dataset returns a single tensor\n",
    "        optimizer.zero_grad()\n",
    "        spikes = spikes.to(device)\n",
    "        spikes_masked, mask = do_masking(spikes, mask_ratio)\n",
    "        preds = net(spikes_masked.float())\n",
    "        loss = criterion(preds[mask], spikes[mask]).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    return float(np.mean(epoch_losses))\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    net: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    criterion: nn.Module,\n",
    "    mask_ratio: float = 0.0,\n",
    "    has_ground_truth: bool = False,\n",
    ") -> Tuple[float, float]:\n",
    "    net.eval()\n",
    "    losses, r2s = [], []\n",
    "    with torch.no_grad():\n",
    "        for spikes, ground_truth in loader:\n",
    "            # Measure performance on the pretext task\n",
    "            spikes = spikes.to(device)\n",
    "            masked_spikes, mask = (\n",
    "                do_masking(spikes, mask_ratio)\n",
    "                if mask_ratio > 0\n",
    "                else (spikes, torch.ones_like(spikes, dtype=torch.bool))\n",
    "            )\n",
    "            preds = net(masked_spikes.float())\n",
    "            loss = criterion(preds[mask], spikes[mask]).mean()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Measure performance on inferring latents (when that makes sense)\n",
    "            if has_ground_truth:\n",
    "                preds = net(spikes.float())\n",
    "                preds_rates = torch.exp(preds)  # Convert from log rates to rates\n",
    "\n",
    "                r2 = calculate_pseudo_r2(\n",
    "                    preds_rates.reshape(-1, spikes.shape[2]),\n",
    "                    ground_truth.to(device).reshape(-1, spikes.shape[2]),\n",
    "                )\n",
    "                r2s.append(r2)\n",
    "            else:\n",
    "                r2s.append(0.0)\n",
    "\n",
    "    return float(np.mean(losses)), float(np.mean(r2s))\n",
    "\n",
    "\n",
    "def train_network(net, data, batch_size, lr, epochs, mask_ratio):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = net.to(device)\n",
    "    train_data = torch.from_numpy(data[\"train_data\"]).int()\n",
    "    val_data = torch.from_numpy(data[\"val_data\"]).int()\n",
    "\n",
    "    has_ground_truth = False\n",
    "    try:\n",
    "        val_truth = torch.from_numpy(data[\"val_truth\"])\n",
    "        has_ground_truth = True\n",
    "        print(\n",
    "            f\"Found ground truth for val data (different from input: {has_ground_truth})\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        # No ground truth available, use the same as input\n",
    "        val_truth = val_data.clone()\n",
    "        print(\"No ground truth for val data available\")\n",
    "    train_truth = torch.from_numpy(data.get(\"train_truth\", train_data.numpy()))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(train_data, train_truth),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=circshift_collate_fn(3),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(val_data, val_truth), batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    criterion = nn.PoissonNLLLoss(reduction=\"none\", log_input=True)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = WarmupCosineSchedule(\n",
    "        optimizer,\n",
    "        warmup_steps=int(epochs * 0.1),  # 10% of total epochs as warmup\n",
    "        t_total=epochs,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs + 1), desc=\"Training\", unit=\"epoch\"):\n",
    "\n",
    "        train_loss = train_one_epoch(\n",
    "            net, train_loader, device, criterion, optimizer, mask_ratio=mask_ratio\n",
    "        )\n",
    "        if epoch % 10 == 0:\n",
    "            val_loss, val_r2 = evaluate(\n",
    "                net,\n",
    "                val_loader,\n",
    "                device,\n",
    "                criterion,\n",
    "                mask_ratio=mask_ratio,\n",
    "                has_ground_truth=has_ground_truth,\n",
    "            )\n",
    "\n",
    "            tqdm.write(\n",
    "                f\"Epoch {epoch:03d} | train NLL {train_loss:.4f} | val NLL {val_loss:.4f} | val R² {val_r2:.4f}\"\n",
    "            )\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = {\n",
    "                    k: v.cpu().detach().clone() for k, v in net.state_dict().items()\n",
    "                }\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # At the end, load the model with the best validation loss\n",
    "    tqdm.write(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    tqdm.write(\"Loading best model state dict\")\n",
    "    net.load_state_dict(best_model)\n",
    "    return net\n",
    "\n",
    "\n",
    "net = SimpleTransformerAutoencoder(\n",
    "    input_dim=n_neurons,\n",
    "    num_layers=6,\n",
    "    num_heads=1,\n",
    "    ffn_dim=64,\n",
    "    dropout=0.7,\n",
    "    max_seq_len=50,\n",
    ")\n",
    "batch_size = 64\n",
    "lr = 2e-3  # Learning rate\n",
    "epochs = 100  # Number of epochs to train\n",
    "mask_ratio = 0.25\n",
    "\n",
    "# Train the network\n",
    "train_network(\n",
    "    net, dataset, batch_size=batch_size, lr=lr, epochs=epochs, mask_ratio=mask_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327aba57",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "The training converged! We can see that the validation loss is decreasing, and the validation R² is increasing.\n",
    "\n",
    "Let's visualize the results on some sample data. We'll take a single validation trial and see how the network embeds the data. You'll note that we don't mask the data in this case: we just take the raw data and look at how the auto-encoder treats it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11214293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which trial to visualize?\n",
    "def visualize_estimated_spike_rates(example_batch, true_rates, estimated_spike_rates):\n",
    "    \"\"\"Visualize the model's predictions against the true rates.\"\"\"\n",
    "    n_batch, nt, n_neurons = example_batch.shape\n",
    "    plt.figure(figsize=(4, 12))\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.imshow(\n",
    "        example_batch[0].detach().cpu().numpy().T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            nt * bin_size,\n",
    "            0,\n",
    "            n_neurons,\n",
    "        ],\n",
    "    )\n",
    "    plt.title(\"Input spikes\")\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.imshow(\n",
    "        true_rates[0].detach().cpu().numpy().T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            nt * bin_size,\n",
    "            0,\n",
    "            n_neurons,\n",
    "        ],\n",
    "    )\n",
    "    plt.title(\"Ground truth latents\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.imshow(\n",
    "        estimated_spike_rates.T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            nt * bin_size,\n",
    "            0,\n",
    "            n_neurons,\n",
    "        ],\n",
    "    )\n",
    "    plt.title(\"Model prediction\")\n",
    "    plt.show()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "visualize_index = 2\n",
    "net.eval()\n",
    "example_batch = (\n",
    "    torch.tensor(dataset[\"val_data\"][visualize_index : visualize_index + 1, :, :])\n",
    "    .int()\n",
    "    .to(device)\n",
    ")\n",
    "true_rates = torch.tensor(\n",
    "    dataset[\"val_truth\"][visualize_index : visualize_index + 1, :, :]\n",
    ").to(device)\n",
    "\n",
    "estimated_spike_log_rates = net(example_batch.float())\n",
    "estimated_spike_rates = torch.exp(estimated_spike_log_rates).detach().cpu().numpy()[0]\n",
    "\n",
    "visualize_estimated_spike_rates(example_batch, true_rates, estimated_spike_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c115274a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "This works rather poorly! The model is not able to reconstruct the spike rates well from the masked data. One reason is that the model has rather low capacity: it has only 6 transformer layers, and the hidden dimension is 29. Its dimensionality is the same as the number of neurons, so it has no room to learn a more complex representation.\n",
    "\n",
    "## Training a model with higher capacity: latent space auto-encoding\n",
    "\n",
    "One simple fix is to add a project the input data to a higher dimensional space, and then project it back to the input dimension at the end. This is a common trick in auto-encoders, and it allows the model to learn a more complex representation of the data. We'll add a linear projection layer at the beginning of the network, and a linear projection layer at the end of the network. \n",
    "\n",
    "Thus:\n",
    "\n",
    "* The input has shape `(batch_size, n_timepoints, n_neurons)`, and we project it to a higher dimensional space `(batch_size, n_timepoints, hidden_dim)` using a linear `input_projection` layer.\n",
    "* The encoding layers then operate on this higher dimensional space, and output a tensor of size `(batch_size, n_timepoints, hidden_dim)`\n",
    "* Finally, we project the output back to the input dimension `(batch_size, n_timepoints, n_neurons)` using an `output_projection` layer.\n",
    "\n",
    "Note that `hidden_dim` can be larger or smaller than `n_neurons`, and we can change it to change the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bd3cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 1,\n",
    "        ffn_dim: int = 256,\n",
    "        dropout: float = 0.5,\n",
    "        max_seq_len: int = 1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(max_seq_len, hidden_dim))\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        def create_encoder_layer() -> nn.TransformerEncoderLayer:\n",
    "            return nn.TransformerEncoderLayer(\n",
    "                d_model=self.hidden_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ffn_dim,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "                norm_first=True,\n",
    "            )\n",
    "\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [create_encoder_layer() for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        # This projects the output back to the input dimension\n",
    "        self.output_projection = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        # This is for regularization. We use dropout at multiple points in the network\n",
    "        self.reset_dropout(dropout)\n",
    "\n",
    "    def reset_dropout(self, dropout: float):\n",
    "        # This is for regularization. We use dropout at multiple points in the network. We have\n",
    "        # a separate method to easily change dropout, eventually, which will be useful for fine-tuning.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_latents=False, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer autoencoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "            return_latents: If True, return the latents (the output of the transformer encoder)\n",
    "            mask: Optional mask tensor of shape (batch_size, seq_len) to apply to the input\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed tensor of shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "\n",
    "        # Project input and add positional encoding\n",
    "        x = self.dropout(x)\n",
    "        x = math.sqrt(self.hidden_dim) * self.input_projection(\n",
    "            x\n",
    "        ) + self.pos_embedding[:seq_len, :].unsqueeze(0)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through transformer encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = self.norm(x)  # Final layer normalization\n",
    "        if return_latents:\n",
    "            x_latents = x.clone()  # Save latents if requested\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        # Project back to input dimension\n",
    "        x = self.output_projection(x)\n",
    "        if return_latents:\n",
    "            return x, x_latents\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4e803",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice the key differences from the previous model:\n",
    "\n",
    "* The input projection layer projects the input data to a higher dimensional space (`hidden_dim`), which is larger than the number of neurons.\n",
    "* The output projection layer projects the output back to the input dimension (`input_dim`).\n",
    "* An additional dropout is applied after the input projection and before the output projection, as well as after the transformer layers.\n",
    "\n",
    "Let's train this model on the same data as before, and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ac775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = TransformerAutoencoder(\n",
    "    input_dim=n_neurons,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    num_heads=1,\n",
    "    ffn_dim=128,\n",
    "    dropout=0.7,\n",
    "    max_seq_len=50,\n",
    ")\n",
    "batch_size = 64\n",
    "lr = 2e-3  # Learning rate\n",
    "epochs = 100  # Number of epochs to train\n",
    "mask_ratio = 0.25\n",
    "\n",
    "# Train the network\n",
    "train_network(\n",
    "    net, dataset, batch_size=batch_size, lr=lr, epochs=epochs, mask_ratio=mask_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221601f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training converged, and the R^2 looks much better than before! Let's visualize the results on some sample data, as we did before.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb322a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "visualize_index = 0\n",
    "example_batch = (\n",
    "    torch.tensor(dataset[\"val_data\"][visualize_index : visualize_index + 1, :, :])\n",
    "    .int()\n",
    "    .to(device)\n",
    ")\n",
    "true_rates = torch.tensor(\n",
    "    dataset[\"val_truth\"][visualize_index : visualize_index + 1, :, :]\n",
    ").to(device)\n",
    "\n",
    "net.eval()\n",
    "estimated_spike_log_rates = net(example_batch.float())\n",
    "estimated_spike_rates = torch.exp(estimated_spike_log_rates).detach().cpu().numpy()[0]\n",
    "visualize_estimated_spike_rates(example_batch, true_rates, estimated_spike_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2045a9b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "That looks much better. An advantage of doing the autoencoding in latent space is that we can flexibly change the latent dimensionality of the network, increasing the capacity of the model without affecting the number of layers. \n",
    "\n",
    "Does the latent space encode something interesting about the dynamical system that generated this data? Transformers have a reputation as black boxes, but nothing prevents us from looking at what's inside the models to learn about how they operate. We can verify this by looking at the weights of the model. Let's use PCA to determine the measure the top PCs of the input and output projection matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_singular_values(Si, So, Vi, Uo):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(Si, \"o-\", label=\"Input embedding singular values\")\n",
    "    plt.xlabel(\"Singular value index\")\n",
    "    plt.title(\"Input embedding singular values\")\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(So, \"o-\", label=\"Output readout singular values\")\n",
    "    plt.xlabel(\"Singular value index\")\n",
    "    plt.title(\"Output readout singular values\")\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(rectify(Vi[:3, :].T))\n",
    "    plt.xlabel(\"Neuron #\")\n",
    "    plt.ylabel(\"Loading\")\n",
    "    plt.title(\"Top 3 input projection singular vectors\")\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(rectify(Uo[:, :3]))\n",
    "    plt.xlabel(\"Neuron #\")\n",
    "    plt.ylabel(\"Loading\")\n",
    "    plt.title(\"Top 3 output projection singular vectors\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "Wi = net.input_projection.weight.detach().cpu().numpy()\n",
    "Wo = net.output_projection.weight.detach().cpu().numpy()\n",
    "\n",
    "Ui, Si, Vi = np.linalg.svd(Wi, full_matrices=False)\n",
    "Uo, So, Vo = np.linalg.svd(Wo, full_matrices=False)\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    \"\"\"Rectify a matrix by setting negative values to zero.\"\"\"\n",
    "    return x * np.sign(x.mean(axis=0, keepdims=True))\n",
    "\n",
    "display_singular_values(Si, So, Vi, Uo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da721e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Notice the singular values of the input and output embedding matrices fall off dramatically after the 2nd (input) or 3rd (output) singular value. Interestingly, the Lorenz dynamical system is 3-dimensional, with 2 dimensions capturing most of the variance; the model has learned to embed the data in a 3-dimensional latent space.\n",
    "\n",
    "Notice also the structure of the input and output projection singular vectors. They are highly structured, and indeed seem to distinguish three groups of neurons: neurons 1-10, neurons 11-15, and neurons 16-29. If you look closely at the ground truth data, you'll notice that indeed the spike rates display this same grouping, which is a reflection of how the data was generated.\n",
    "\n",
    "Thus, it appears the model has leveraged the fact that the data lies on a low-rank manifold, a core assumption of many models that learn structure from neural data, including PCA, LFADS, and GPFA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efcf8f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Real data: `mc_maze`\n",
    "\n",
    "Now that we've gotten a handle on working with toy data, let's switch over to real data. One core application of foundation models for neuroscience is for BCI decoding. Let's train and apply our model to a BCI decoding experiment.\n",
    "\n",
    "In the `mc_maze` series of datasets (`mc_maze`, `mc_maze_large`, `mc_maze_medium`, `mc_maze_small`; [Churchland et al. 2010](https://pubmed.ncbi.nlm.nih.gov/21040842/)), a monkey completes a reaching task where he needs to trace with his finger on a touchscreen from a start position to an end position, avoiding the maze walls. Neurons are recorded in premotor cortex (PMd) and in M1. \n",
    "\n",
    "![mc_maze dataset](images/mc_maze.png)\n",
    "\n",
    "The data is similarly structured to the Lorenz dataset, with a few key differences:\n",
    "\n",
    "* `val_truth`: \"Ground truth\" data for the true underlying spike rates. Unlike the Lorenz dataset, we can never truly know what this is. Instead, we estimate it from taking an average over similar trials and applying a 50 ms smoothing window.\n",
    "* `train_behavior` and `val_behavior`: Aligned behavior, an array of shape `(n_trials, 2)`. This corresponds to the monkey's arm velocity, in `m/s`. We can use this to train a BCI decoder.\n",
    "\n",
    "Let's start by visualizing this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c469c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"mc_maze_medium\")\n",
    "\n",
    "def visualize_mc_maze_data(trial_spikes, trial_truth, trial_behavior):\n",
    "    \"\"\"Visualize a single trial of the mc_maze dataset.\"\"\"\n",
    "    # One figure, two axes stacked vertically\n",
    "    fig, (ax_top, ax_bottom) = plt.subplots(\n",
    "        nrows=2,\n",
    "        ncols=2,  # two rows, one column\n",
    "        gridspec_kw={\n",
    "            \"height_ratios\": [1, 2],\n",
    "            \"width_ratios\": [1, 1],\n",
    "        },  # 1 : 2  ⇒ top = ⅓, bottom = ⅔\n",
    "        figsize=(6, 8),  # any size you like\n",
    "        sharex=False,  # optional: share the x-axis\n",
    "    )\n",
    "\n",
    "    nt, n_neurons = trial_spikes.shape[0], trial_spikes.shape[1]\n",
    "\n",
    "    bin_size = 0.01\n",
    "\n",
    "    ax_top[0].plot(\n",
    "        np.arange(nt) * bin_size,\n",
    "        trial_behavior,\n",
    "    )\n",
    "    ax_top[0].legend([\"Velocity (x)\", \"Velocity (y)\"])\n",
    "    ax_top[0].set_xlim(0, nt * bin_size)\n",
    "    ax_top[0].set_ylim(-1, 1)\n",
    "    ax_top[0].set_xlabel(\"Time (s)\")\n",
    "    ax_top[0].set_ylabel(\"Velocity (m/s)\")\n",
    "\n",
    "    ax_top[1].plot(\n",
    "        bin_size * np.cumsum(trial_behavior[:, 0]),\n",
    "        bin_size * np.cumsum(trial_behavior[:, 1]),\n",
    "        \"-.\",\n",
    "    )\n",
    "    ax_top[1].set_xlabel(\"x position (m)\")\n",
    "    ax_top[1].set_ylabel(\"y position (m)\")\n",
    "    ax_top[1].set_xlim([-0.2, 0.2])\n",
    "    ax_top[1].set_ylim([-0.2, 0.2])\n",
    "    ax_top[1].plot(\n",
    "        np.cumsum(trial_behavior[:, 0])[-1],\n",
    "        np.cumsum(trial_behavior[:, 1])[-1],\n",
    "        \"gx\",\n",
    "    )  # mark the end\n",
    "    ax_top[1].plot(0, 0, \"ro\")\n",
    "\n",
    "    ax_bottom[0].imshow(\n",
    "        trial_spikes.T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            nt * bin_size,\n",
    "            0,\n",
    "            n_neurons,\n",
    "        ],\n",
    "    )\n",
    "    ax_bottom[0].set_xlabel(\"Time (s)\")\n",
    "    ax_bottom[0].set_ylabel(\"Neuron #\")\n",
    "    ax_bottom[0].set_title(\"Spikes\")\n",
    "\n",
    "    ax_bottom[1].imshow(\n",
    "        trial_truth.T,\n",
    "        cmap=\"gray_r\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            0,\n",
    "            nt * bin_size,\n",
    "            0,\n",
    "            n_neurons,\n",
    "        ],\n",
    "    )\n",
    "    ax_bottom[1].set_xlabel(\"Time (s)\")\n",
    "    ax_bottom[1].set_ylabel(\"Neuron #\")\n",
    "    ax_bottom[1].set_title(\"Ground truth (smoothed data)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "trial_spikes = dataset[\"val_data\"][0, :, :]  # First trial spikes\n",
    "trial_truth = dataset[\"val_truth\"][0, :, :]\n",
    "trial_behavior = dataset[\"val_behavior\"][0, :]  # First trial behavior\n",
    "visualize_mc_maze_data(trial_spikes, trial_truth, trial_behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2298ac",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "As we did with the Lorenz dataset, let's learn a masked autoencoder on this data. We'll use the same `TransformerAutoencoder` class we defined earlier, but with different hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility. This model is somewhat finicky, and doesn't always converge. In practice\n",
    "# one might want to pick several random seeds and pick the best outcome. That would take a long time, so here we just set a fixed seed.\n",
    "torch.manual_seed(42)\n",
    "net = TransformerAutoencoder(\n",
    "    input_dim=dataset[\"train_data\"].shape[2],\n",
    "    hidden_dim=256,\n",
    "    num_layers=6,\n",
    "    num_heads=2,\n",
    "    ffn_dim=128,\n",
    "    dropout=0.7,\n",
    "    max_seq_len=70,\n",
    ")\n",
    "batch_size = 32\n",
    "lr = 1e-2  # Learning rate\n",
    "epochs = 1000  # Number of epochs to train\n",
    "mask_ratio = 0.25\n",
    "\n",
    "# Train the network\n",
    "train_network(\n",
    "    net, dataset, batch_size=batch_size, lr=lr, epochs=epochs, mask_ratio=mask_ratio\n",
    ")\n",
    "\n",
    "# To easily recover the model once we've trained it.\n",
    "saved_state_dict = {k: v.cpu().detach().clone() for k, v in net.state_dict().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081b71f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "That worked! Now we have a reasonable representation of the data. Now, how do we use this hidden representation for BCI decoding with transformers? One approach is to use **transfer learning**: first train a transformer on a masked autoencoding task, then fine-tune it on the BCI decoding task. \n",
    "\n",
    "The key step is to use the latents from the masked autoencoder as input to the BCI decoder. By latents, we mean the output of the transformer encoder layers, before the output projection layer. We'll grab these latents using TransformerAutoencoder's `return_latents` argument and train a lightweight decoder on top of them.\n",
    "\n",
    "Let's create a lightweight shim on top of the TransformerAutoencoder that allows us to train a BCI decoder on top of the latents. We'll use the simplest setup, where the sampling rate of the behavior is the same as the sampling rate of the spikes, and spikes and behavior are already aligned temporally. In that case, we can just use a linear layer to decode the behavior from the latents: one token = one timepoint = one behavioral sample. Note that you could use a more powerful decoder like another transformer, or use sophisticated mechanisms to handle different sampling rates than the spikes---see the references for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb441c8e",
   "metadata": {},
   "source": [
    "class TransformerWithDecoder(nn.Module):\n",
    "    \"\"\"Combines pretrained PM Transformer with behavior decoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer: nn.Module,\n",
    "        behavior_dim: int,\n",
    "        freeze_transformer: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "        hidden_dim = transformer.hidden_dim\n",
    "        self.decoder = nn.Linear(in_features=hidden_dim, out_features=behavior_dim)\n",
    "\n",
    "        self.set_freeze_transformer(freeze_transformer)\n",
    "\n",
    "    def set_freeze_transformer(self, freeze: bool):\n",
    "        \"\"\"Freeze or unfreeze transformer parameters.\"\"\"\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input spikes (B, T, neurons)\n",
    "        Returns:\n",
    "            Tuple of (reconstructed spikes, decoded behavior)\n",
    "        \"\"\"\n",
    "        # Get internal representations\n",
    "        _, h = self.transformer.forward(x=x, return_latents=True)\n",
    "\n",
    "        # Decode behavior from representations\n",
    "        behavior = self.decoder(h)\n",
    "\n",
    "        return behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970b7fa",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Now we're ready to train this. We set up another training loop. Note that this time, our criterion will be the MSE loss, since we're predicting continuous behavior values. We also use far more conservative dropout rate.\n",
    "\"\"\"\n",
    "def finetune_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_r2s = []\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "\n",
    "    for spikes, behavior in loader:\n",
    "        spikes = spikes.to(device).float()\n",
    "        behavior = behavior.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred_behavior = model(spikes)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion_mse(\n",
    "            pred_behavior.reshape(-1, pred_behavior.shape[-1]),\n",
    "            behavior.reshape(-1, behavior.shape[-1]),\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        epoch_losses.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            r2 = calculate_pseudo_r2(pred_behavior.reshape((-1, pred_behavior.shape[2])), behavior.reshape((-1, pred_behavior.shape[2])))\n",
    "            epoch_r2s.append(r2)\n",
    "\n",
    "    return float(np.mean(epoch_losses)), float(np.mean(epoch_r2s))\n",
    "\n",
    "\n",
    "def finetune_evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    r2s = []\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spikes, behavior in loader:\n",
    "            spikes = spikes.to(device).float()\n",
    "            behavior = behavior.to(device).float()\n",
    "\n",
    "            # Forward pass\n",
    "            pred_behavior = model(spikes)\n",
    "\n",
    "            # Compute metrics\n",
    "            loss = criterion_mse(pred_behavior, behavior)\n",
    "            r2 = calculate_pseudo_r2(pred_behavior.reshape((-1, pred_behavior.shape[2])), behavior.reshape((-1, pred_behavior.shape[2])))\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            r2s.append(r2)\n",
    "\n",
    "    return float(np.mean(losses)), float(np.mean(r2s))\n",
    "\n",
    "\n",
    "def finetune_bci_decoder(\n",
    "    model: nn.Module,\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    epochs,\n",
    "):\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(dataset[\"train_data\"]).int(),\n",
    "            torch.from_numpy(dataset[\"train_behavior\"]).float(),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(dataset[\"val_data\"]).int(),\n",
    "            torch.from_numpy(dataset[\"val_behavior\"]).float(),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # Create new optimizer with all parameters\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr * 0.1,  # Use lower learning rate for fine-tuning\n",
    "    )\n",
    "    scheduler = WarmupCosineSchedule(\n",
    "        optimizer,\n",
    "        warmup_steps=int(epochs * 0.1),  # 10% of total epochs as warmup\n",
    "        t_total=epochs,\n",
    "    )\n",
    "    best_val_r2 = -float(\"inf\")\n",
    "    for epoch in tqdm(range(1, epochs + 1), desc=\"Training\", unit=\"epoch\"):\n",
    "        train_loss, train_r2 = finetune_one_epoch(\n",
    "            model, train_loader, device, optimizer\n",
    "        )\n",
    "        scheduler.step()\n",
    "        if epoch % 10 == 0:\n",
    "            val_loss, val_r2 = finetune_evaluate(model, val_loader, device)\n",
    "            tqdm.write(\n",
    "                f\"Epoch {epoch:03d} | \"\n",
    "                f\"train loss {train_loss:.4f} | train R² {train_r2:.4f} | \"\n",
    "                f\"val loss {val_loss:.4f} | val R² {val_r2:.4f}\"\n",
    "            )\n",
    "            if val_r2 > best_val_r2:\n",
    "                best_val_r2 = val_r2\n",
    "                best_model = {\n",
    "                    k: v.cpu().detach().clone() for k, v in model.state_dict().items()\n",
    "                }\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, best_val_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b703ab",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Note that the training loop is similar to the one we used for training the masked autoencoder, but now we use MSE loss instead of Poisson NLL loss, and we don't mask the data. The model is trained to predict the behavior from the latents, which are the output of the transformer encoder.\n",
    "\n",
    "Now we're ready to train the model. We'll use two different training modes:\n",
    "* Frozen encoder: We freeze the transformer encoder and only train the decoder. This is useful for transfer learning, where we want to leverage the pretrained representations.\n",
    "* End-to-end training: We unfreeze the transformer encoder and train the entire model end-to-end. This is useful for fine-tuning the model on the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = []\n",
    "for frozen_encoder in [True, False]:\n",
    "    # Create the model with frozen transformer\n",
    "    print(\n",
    "        f\"Training with {'frozen' if frozen_encoder else 'unfrozen'} transformer encoder\"\n",
    "    )\n",
    "\n",
    "    # Load the pretrained transformer autoencoder.\n",
    "    net.load_state_dict(saved_state_dict)\n",
    "    net.reset_dropout(0.1)  # Reset dropout to a lower value for fine-tuning\n",
    "    model = TransformerWithDecoder(\n",
    "        transformer=net,\n",
    "        behavior_dim=dataset[\"train_behavior\"].shape[2],\n",
    "        freeze_transformer=frozen_encoder,  # Freeze the transformer encoder\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    batch_size = 64\n",
    "    lr = 5e-2  # Learning rate\n",
    "    epochs = 500  # Number of epochs to train\n",
    "    # Train the model with frozen transformer\n",
    "    model, best_r2 = finetune_bci_decoder(\n",
    "        model,\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"method\": (\n",
    "                \"linear on top of frozen mc_maze_medium checkpoint\"\n",
    "                if frozen_encoder\n",
    "                else \"fine-tune from mc_maze_medium checkpoint (end-to-end)\"\n",
    "            ),\n",
    "            \"best_val_r2\": best_r2,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33253fc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Great! We see that the performance of the BCI decoder is quite a bit better when the model is trained end-to-end than when it is trained with a frozen encoder. This is expected, as the model can adapt the representations to the specific task.\n",
    "\n",
    "But how well does the model perform on the BCI decoding task compared to alternatives? We have to compare our model against baselines!\n",
    "\n",
    "Let's create two baselines:\n",
    "\n",
    "* Linear decoder: a simple linear decoder on top of smoothed spikes.\n",
    "* Transformer decoder trained from scratch: a (smaller) transformer trained from scratch on the BCI decoding task, without leveraging the pretraining task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_smooth_1d(x, sigma=5):\n",
    "    \"\"\"\n",
    "    Apply Gaussian smoothing along the time dimension.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch, time, neurons)\n",
    "        sigma: Standard deviation of Gaussian kernel\n",
    "\n",
    "    Returns:\n",
    "        Smoothed tensor of same shape as input\n",
    "    \"\"\"\n",
    "    batch, time, neurons = x.shape\n",
    "\n",
    "    # Create Gaussian kernel\n",
    "    # Kernel size should be odd and large enough to capture the Gaussian\n",
    "    kernel_size = int(6 * sigma + 1)  # 6 sigma captures 99.7% of distribution\n",
    "    if kernel_size % 2 == 0:\n",
    "        kernel_size += 1  # Ensure odd size\n",
    "\n",
    "    # Create 1D Gaussian kernel\n",
    "    kernel = torch.arange(kernel_size, dtype=torch.float32)\n",
    "    kernel = kernel - kernel_size // 2  # Center around 0\n",
    "    kernel = torch.exp(-0.5 * (kernel / sigma) ** 2)\n",
    "    kernel = kernel / kernel.sum()  # Normalize\n",
    "\n",
    "    # Move kernel to same device as input\n",
    "    kernel = kernel.to(x.device)\n",
    "\n",
    "    # Reshape for conv1d: (batch, time, neurons) -> (batch * neurons, 1, time)\n",
    "    x_reshaped = x.permute(0, 2, 1).reshape(batch * neurons, 1, time)\n",
    "\n",
    "    # Reshape kernel for conv1d: needs shape (out_channels, in_channels, kernel_size)\n",
    "    kernel = kernel.view(1, 1, -1)\n",
    "\n",
    "    # Apply convolution with padding to maintain time dimension\n",
    "    padding = kernel_size // 2\n",
    "    x_smoothed = F.conv1d(x_reshaped, kernel, padding=padding)\n",
    "\n",
    "    # Reshape back: (batch * neurons, 1, time) -> (batch, time, neurons)\n",
    "    x_smoothed = x_smoothed.view(batch, neurons, time).permute(0, 2, 1)\n",
    "\n",
    "    return x_smoothed\n",
    "\n",
    "\n",
    "class SmoothDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sigma):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.output_decoder = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the smooth transformer.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed tensor of shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        # Apply Gaussian smoothing\n",
    "        x_smoothed = gaussian_smooth_1d(x, self.sigma)\n",
    "\n",
    "        # Pass through the transformer\n",
    "        return self.output_decoder(x_smoothed)\n",
    "\n",
    "\n",
    "# Train the model with a smooth decoder\n",
    "\n",
    "batch_size = 64\n",
    "lr = 5e-2  # Learning rate\n",
    "epochs = 500  # Number of epochs to train\n",
    "# Train the model with frozen transformer\n",
    "model = SmoothDecoder(\n",
    "    input_dim=dataset[\"train_data\"].shape[2],\n",
    "    output_dim=dataset[\"train_behavior\"].shape[2],\n",
    "    sigma=5,  # Standard deviation for Gaussian smoothing\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "model, val_r2 = finetune_bci_decoder(\n",
    "    model,\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "results.append({\"method\": \"linear on top of smoothed spikes\", \"best_val_r2\": val_r2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34934fc1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now let's train a transformer decoder from scratch. This is a smaller transformer than the one we used for pretraining, since we don't need as much capacity to decode the behavior. We'll use the same training loop as before, but this time we'll train the entire model end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a transformer decoder from scratch\n",
    "lr = 1e-1  # Learning\n",
    "epochs = 1000  # Number of epochs to train\n",
    "supervised_net = TransformerAutoencoder(\n",
    "    input_dim=dataset[\"train_data\"].shape[2],\n",
    "    hidden_dim=32,\n",
    "    num_layers=4,\n",
    "    num_heads=1,\n",
    "    ffn_dim=128,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Train with a transformer decoder from scratch\n",
    "model = TransformerWithDecoder(\n",
    "    transformer=supervised_net,\n",
    "    behavior_dim=dataset[\"train_behavior\"].shape[2],\n",
    "    freeze_transformer=False,  # Unfreeze the transformer encoder\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "model, val_r2 = finetune_bci_decoder(\n",
    "    model,\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "results.append({\"method\": \"supervised\", \"best_val_r2\": val_r2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f891362",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We notice a few important trends:\n",
    "\n",
    "* The fine-tuned model performs better than the frozen model, which is expected since it can adapt the representations to the specific task.\n",
    "* The smooth linear decoder underperforms compared to the fine-tuned model, which is expected since it doesn't have as much capacity as the transformer decoders.\n",
    "* The supervised transformer decoder performs a bit better then the fine-tuned model.\n",
    "\n",
    "This is a bit disappointing: we went through the trouble of pretraining because we expected that it would yield improved decoding. It's not too surprising, however: the `mc_maze_medium` dataset is fairly small, and so the pretraining doesn't learn a sufficiently good representation to obtain a significant lift on the decoding task. The promise of a foundation model is to pretrain on a **large** dataset---potentially many orders of magnitude larger than the fine-tuning task---, and then fine-tune on the smaller dataset. Let's do that next.\n",
    "\n",
    "# Transfer learning from a larger model\n",
    "\n",
    "To train a foundation model for neuroscience, we need a large dataset. `mc_maze_medium` only contains about two hundred trials, which is not enough to train a good representation. Instead, we'll use the `mc_maze` dataset, which contains about 10 times that. This is still fairly small by foundation model standards, but it contains thousands of trials from which to learn a good representation. In the real world, you would typically train on much larger datasets like you might find on DANDI. But this is enough to demonstrate the core ideas.\n",
    "\n",
    "I've already trained a model on the `mc_maze` dataset, and saved the weights to a file. You can find the weights in the `scripts` directory of this repository, in the file `mc_maze_tuned.pt`. If you wanted to train the model yourself, you could do so by running the `scripts/train_autoencoder.py` script, specifically with these parameters: \n",
    "\n",
    "```\n",
    "python train_autoencoder.py --hidden-dim 256 --dropout 0.7 --epochs 10000 --pos-encoding learned --mask-ratio 0.25 --dataset mc_maze --checkpoint mc_maze_tuned.pt --projection linear --lr 2e-3 --use-wandb --model pm --num-heads 2 --num-layers 6 --context-forward 0 --context-backward 0 --ffn-dim 256 --batch-size 64 --delta 3\n",
    "```\n",
    "\n",
    "It will take about an hour to train. Here we'll just load the pretrained model weights and transfer them to the `mc_maze_medium` dataset. There's one very important thing we'll need to take care of, however:\n",
    "\n",
    "## Adapting the model to a new dataset\n",
    "\n",
    "One challenge with using pretrained models in neuroscience is that the neurons recorded vary from dataset to dataset. That's quite different than a language model where the vocabulary will stay constant when using across datasets. \n",
    "\n",
    "We need to carefully adapt our pretrained model to the new dataset. Conceptually, we think that *what is conserved* across datasets is how neural data is represented in latent space. What is *not conversed* is the projections from the input space to the latent space, and back.\n",
    "\n",
    "In fact, the `mc_maze` dataset and `mc_maze_medium` datasets have different numbers of neurons. This means that the input and output projection layers of the pretrained model will not match the new dataset, and we have to learn them from scratch.\n",
    "\n",
    "For example, the `mc_maze` dataset contains 29 neurons, while the `mc_maze_medium` dataset contains only 16 neurons. This means that we can't use the entire model weights as they are: we'll need to adapt the input and output projection layers to match the new number of neurons. We'll need to carefully swap the input and output projection layers to match the new number of neurons.\n",
    "\n",
    "Then we'll go ahead and train the model and see how well it performs on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de24ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pretrained model\n",
    "net = TransformerAutoencoder(\n",
    "    input_dim=dataset[\"train_data\"].shape[2],\n",
    "    hidden_dim=256,\n",
    "    num_layers=6,\n",
    "    num_heads=2,\n",
    "    ffn_dim=256,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=70,\n",
    ")\n",
    "\n",
    "# Here's the tricky part: we'll overwrite the input and output projection layers to match the new number of neurons\n",
    "torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "pretrained_model_path = \"checkpoints/mc_maze_tuned.pt\"  # Adjust this path\n",
    "ckpt = torch.load(pretrained_model_path, map_location=device)\n",
    "state_dict = ckpt[\"model_state_dict\"]\n",
    "try:\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading state dict: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ceba80",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice that this returned an error, because the input and output projection layers of the model pretrained on `mc_maze` don't match the new number of neurons in `mc_maze_medium`. We'll need to overwrite them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input and output projection layers need to match the new number of neurons, so we'll just use the weights that are already in the model.\n",
    "state_dict[\"input_projection.weight\"] = net.input_projection.weight.data.detach()\n",
    "state_dict[\"output_projection.weight\"] = net.output_projection.weight.data.detach()\n",
    "state_dict[\"output_projection.bias\"] = net.output_projection.bias.data.detach()\n",
    "net.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c49f97",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we're ready to train the model on the new dataset! The input and output projection layers will be trained as part of the fine-tuning process.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34303fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 5e-2  # Learning rate\n",
    "epochs = 500  # Number of epochs to train\n",
    "\n",
    "net = net.to(device)\n",
    "model = TransformerWithDecoder(\n",
    "    transformer=net,\n",
    "    behavior_dim=dataset[\"train_behavior\"].shape[2],\n",
    "    freeze_transformer=False,  # Unfreeze the transformer encoder\n",
    ").to(device)\n",
    "model, val_r2 = finetune_bci_decoder(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    ")\n",
    "results.append(\n",
    "    {\"method\": \"fine-tune from mc_maze checkpoint (end-to-end)\", \"best_val_r2\": val_r2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b2a30",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "And there we have it: a model, trained from scratch on a large dataset, that can be adapted to a smaller dataset with a few lines of code. This is the power of foundation models for neuroscience: they allow us to leverage large datasets and transfer learning to build powerful models that can be adapted to new tasks with minimal effort.\n",
    "\n",
    "Let's see all the scores of the different model variants we've tried on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(results).set_index(\"method\").sort_values(\"best_val_r2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc7353",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Where foundation models especially shine is when we have very large pretraining datasets, and very small fine-tuning datasets. You can try, for example, to finetune the `mc_maze` checkpoint on one half of the `mc_maze_small` dataset (only ~40 trials). You'll see that pretraining makes a big difference, and the model can achieve a much higher R² than training from scratch.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd0f4c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Causal decoding\n",
    "\n",
    "We just have one more concept to cover: causal decoding. Thus far, we've spikes from an entire trial to predict behavior. If we wanted to use this for online BCI decoding, we could only use spikes that have happened thus far. In other words, we want our predicted behavior $y_T$ to only depend on spikes that have happened up to time `T`:\n",
    "\n",
    "$$y_T = = f([s_1, s_2, s_3, ..., s_T])$$\n",
    "\n",
    "The fix is to use a **causal** transformer decoder. To prevent information from flowing from the future to the point, we'll use a mask that tells the model to only use the first token to reconstruct the first token; the first two tokens to reconstruct the second token; the first three tokens to reconstruct the third token, and so on. This is a causal decoder, and it allows us to use the model for online BCI decoding.\n",
    "\n",
    "This mask, which is a lower triangular matrix, can be created with the `torch.tril` function. We'll use this to create a causal transformer decoder that can be used for online BCI decoding.\n",
    "\n",
    "Confusingly, the mask should be set to False to signal that information is allowed to flow, and True to signal that information should be *masked out*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~torch.tril(torch.ones(50, 50, dtype=torch.bool))\n",
    "plt.imshow(mask, cmap=\"gray\", aspect=\"auto\")\n",
    "plt.xlabel(\"Time (bins) – information flows from\")\n",
    "plt.ylabel(\"Time (bins) - information flows to\")\n",
    "plt.title(\"Example causal mask: black = allowed, white = masked out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af32be99",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "To leverage the causal mask, we simply need to pass it to the transformer layers in the forward pass, like so:\n",
    "\n",
    "```\n",
    "# Instance of a transformer encoder layer\n",
    "layer = nn.TransformerEncoderLayer(...)\n",
    "# Our mask, as above.\n",
    "mask = ~torch.tril(torch.ones(50, 50, dtype=torch.bool))\n",
    "# When we forward an input to our layer, we pass along the mask so information only flows between the desired tokens.\n",
    "layer(x, mask=mask)\n",
    "```\n",
    "\n",
    "We had already set up a parameter in the forward function in the `TransformerAutoencoder` implementation to make this easy. We can simply pass the mask to the `forward` method of the transformer, and it will take care of applying it to all transformer layers.\n",
    "\n",
    "Now let's implement the causal transformer decoder. We'll use the same architecture as before, but we'll add a causal mask to the transformer layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3084d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CausalTransformerWithDecoder(TransformerWithDecoder):\n",
    "    \"\"\"Combines pretrained PM Transformer with behavior decoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer: nn.Module,\n",
    "        behavior_dim: int,\n",
    "        freeze_transformer: bool = True,\n",
    "    ):\n",
    "        super().__init__(transformer, behavior_dim, freeze_transformer)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input spikes (B, T, neurons)\n",
    "        Returns:\n",
    "            Tuple of (reconstructed spikes, decoded behavior)\n",
    "        \"\"\"\n",
    "        # Generate a causal mask\n",
    "        mask = ~torch.tril(torch.ones((x.shape[1], x.shape[1]), dtype=torch.bool)).to(\n",
    "            x.device\n",
    "        )\n",
    "        # Get internal representations\n",
    "        _, h = self.transformer.forward(x=x, return_latents=True, mask=mask)\n",
    "\n",
    "        # Decode behavior from representations\n",
    "        behavior = self.decoder(h)\n",
    "\n",
    "        return behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00244de5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we're ready to train the causal transformer decoder. We'll use the same training loop as before, but this time we'll pass the causal mask to the transformer layers. We *could* start by pretraining the model on the `mc_maze` dataset with the causal mask, but since we already have a pretrained model, we'll just adapt it to the new dataset and train the causal decoder on top of it. This turns out to be highly effective.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "net = TransformerAutoencoder(\n",
    "    input_dim=dataset[\"train_data\"].shape[2],\n",
    "    hidden_dim=256,\n",
    "    num_layers=6,\n",
    "    num_heads=2,\n",
    "    ffn_dim=256,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=70,\n",
    ")\n",
    "\n",
    "torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "pretrained_model_path = \"checkpoints/mc_maze_tuned.pt\"  # Adjust this path\n",
    "ckpt = torch.load(pretrained_model_path, map_location=device)\n",
    "state_dict = ckpt[\"model_state_dict\"]\n",
    "state_dict[\"input_projection.weight\"] = net.input_projection.weight.data.detach()\n",
    "state_dict[\"output_projection.weight\"] = net.output_projection.weight.data.detach()\n",
    "state_dict[\"output_projection.bias\"] = net.output_projection.bias.data.detach()\n",
    "net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "batch_size = 64\n",
    "lr = 5e-2  # Learning rate\n",
    "epochs = 500  # Number of epochs to train\n",
    "\n",
    "net = net.to(device)\n",
    "causal_model = CausalTransformerWithDecoder(\n",
    "    transformer=net,\n",
    "    behavior_dim=dataset[\"train_behavior\"].shape[2],\n",
    "    freeze_transformer=False,  # Unfreeze the transformer encoder\n",
    ").to(device)\n",
    "causal_model, val_r2 = finetune_bci_decoder(\n",
    "    model=causal_model,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cf631",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we've successfully implemented a causal transformer decoder. This allows us to use the model for online BCI decoding, where we can only use spikes that have happened thus far to predict behavior. Let's verify that the model behaves as expected. We'll do this by passing in a sequence of spikes and checking that the model only uses information from the past to predict the future.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 1\n",
    "spikes = (\n",
    "    torch.from_numpy(dataset[\"train_data\"][sample_idx : sample_idx + 1, :, :])\n",
    "    .to(device)\n",
    "    .float()\n",
    ")\n",
    "# Now create a perturbed version of spikes, where there's some noise in the second half of the trial..\n",
    "spikes_perturbed = spikes.clone()\n",
    "spikes_perturbed[:, spikes.shape[1] // 2 :, :] += (\n",
    "    (torch.rand_like(spikes[:, spikes.shape[1] // 2 :, :]) * 3).int().float()\n",
    ")\n",
    "\n",
    "causal_model.eval()\n",
    "model.eval()\n",
    "\n",
    "behavior = (\n",
    "    torch.from_numpy(dataset[\"train_behavior\"][sample_idx : sample_idx + 1, :, :])\n",
    "    .to(device)\n",
    "    .float()\n",
    ")\n",
    "\n",
    "# predicted_behavior = model(spikes)\n",
    "predicted_behavior_causal = causal_model(spikes)\n",
    "predicted_behavior_acausal = model(spikes)\n",
    "predicted_behavior_causal_perturbed = causal_model(spikes_perturbed)\n",
    "predicted_behavior_acausal_perturbed = model(spikes_perturbed)\n",
    "\n",
    "# Draw the results\n",
    "rg = np.arange(spikes.shape[1]) * bin_size\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.subplot(5, 1, 1)\n",
    "plt.plot(\n",
    "    rg, behavior[0, :, :].detach().cpu().numpy(), label=\"Causal predicted behavior (x)\"\n",
    ")\n",
    "plt.box(False)\n",
    "plt.title(\"True behavior\")\n",
    "\n",
    "plt.subplot(5, 1, 2)\n",
    "plt.plot(\n",
    "    rg,\n",
    "    predicted_behavior_causal[0, :, :].detach().cpu().numpy(),\n",
    "    label=\"Causal predicted behavior (x)\",\n",
    ")\n",
    "plt.box(False)\n",
    "plt.title(\"Causally decoded behavior (clean)\")\n",
    "\n",
    "plt.subplot(5, 1, 3)\n",
    "plt.plot(\n",
    "    rg,\n",
    "    predicted_behavior_acausal[0, :, :].detach().cpu().numpy(),\n",
    "    label=\"Causal predicted behavior (x)\",\n",
    ")\n",
    "plt.box(False)\n",
    "plt.title(\"Acausally decoded behavior (clean)\")\n",
    "\n",
    "plt.subplot(5, 1, 4)\n",
    "plt.plot(\n",
    "    rg,\n",
    "    predicted_behavior_causal_perturbed[0, :, :].detach().cpu().numpy(),\n",
    "    label=\"Causal predicted behavior (x)\",\n",
    ")\n",
    "plt.axvline(\n",
    "    x=spikes.shape[1] // 2 * bin_size,\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Perturbation point\",\n",
    ")\n",
    "plt.box(False)\n",
    "plt.title(\"Causally decoded behavior (perturbed)\")\n",
    "\n",
    "plt.subplot(5, 1, 5)\n",
    "plt.plot(\n",
    "    rg,\n",
    "    predicted_behavior_acausal_perturbed[0, :, :].detach().cpu().numpy(),\n",
    "    label=\"Causal predicted behavior (x)\",\n",
    ")\n",
    "plt.axvline(\n",
    "    x=spikes.shape[1] // 2 * bin_size,\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Perturbation point\",\n",
    ")\n",
    "plt.box(False)\n",
    "plt.title(\"Acausally decoded behavior (perturbed)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Velocity (m/s)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fb554",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice that the causal decoder's output is only affected in the second half of the trial, after the perturbation, whereas the acausal decoder is affected everywhere. We've thus successfully implemented a causal transformer decoder that can be used for online BCI decoding. This allows us to use the model in real-time applications, where we can only use spikes that have happened thus far to predict behavior.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this tutorial, we've covered the following topics:\n",
    "\n",
    "* How to implement a masked autoencoder using transformers\n",
    "* How to train a transformer autoencoder on a toy dataset\n",
    "* How to apply the transformer autoencoder to real data\n",
    "* How to train a BCI decoder on top of the transformer latents\n",
    "* How to adapt a pretrained transformer model to a new dataset\n",
    "* How to implement a causal transformer decoder for online BCI decoding\n",
    "\n",
    "So how can you take this to the next level? The literature is rife with ideas for how to improve this model. Here are a few suggestions:\n",
    "\n",
    "* Use a more sophisticated tokenization scheme, such as patching, spike-based tokenization, or latent space tokenization\n",
    "* Use a more sophisticated pretext task, such as next-token prediction, or contrastive learning\n",
    "* Use a more sophisticated decoder, such as another transformer or a recurrent neural network\n",
    "* Use a bigger model on a bigger dataset\n",
    "\n",
    "When you're ready to take that leap, check out the tutorial from Eva Dyer's group on [Foundation Models for Neuroscience](https://colab.research.google.com/github/evadyer/foundation_models_for_neuroscience/blob/main/01_foundation_models_for_neuroscience.ipynb). It covers many of these ideas in more detail, and provides a great starting point for your own research.\n",
    "Here are a few other relevant references:\n",
    "\n",
    "* [Cosyne tutorial](https://cosyne-tutorial-2025.github.io/)\n",
    "* [Blog post](https://www.neuroai.science/p/foundation-models-for-neuroscience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71faf306",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "TODO:\n",
    "\n",
    "* Add more references to the relevant literature\n",
    "* Check in on whether transformers will be explained by the time I give my lecture\n",
    "* Turn into more active exercises\n",
    "* Ask Claude several times how to make this better"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "cellView,-all",
   "cell_metadata_json": true,
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
