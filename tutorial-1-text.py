# %% [markdown]
"""
# Tutorial – building an NDT-style autoencoder from scratch

The goal of this notebook is to guide you through the *building blocks* of foundation models for neuroscience. Foundation models are trained on large-scale data in an unsupervised way, and can be adapted (fine-tuned, or steered using in-context learning) for use for a variety of downstream tasks. Training a foundation model typically involves the use of large-scale compute, but don’t let that scare you away! We can cover core building blocks, like transformers, tokenization, training and fine-tuning in a short tutorial. 

We will train an NDT-1-stitch style model from scratch. NDT-1 (neural data transformer-1) is a model that is trained to predict missing spike data via a masked auto-encoder. It takes in spike data and recovers spike rates. The base NDT-1 is specific to a session: it’s not a foundation model. It’s conceptually similar to models like LFADS or GPFA that attempt to find latents from spike data.

However, a simple extension (NDT-1-stitch) can stitch sessions from different subjects and experiments together. That allows it to be trained at large scale as a foundation model. While there are now many more effective foundation models for neuroscience that can be used for spike data (e.g. POYO), NDT-1-stitch covers most of the relevant ingredients to build a successful foundation model:

* Building transformers that can read and embed spike data
* Creating a self-supervised pretraining task that learns good representations of neural data
* Training a model
* Tracking the performance of a model during self-supervised pretraining
* Building embedding and readout mechanisms that can be used to adapt to different sessions and brains
* Fine-tuning a model trained on one task to another task

Let’s train an NDT-1-stitch style model from scratch. 

## A toy task: the Lorenz dataset

We’ll start by applying our model to artificially generated data. Because the data is artificially generated, we know exactly what the latents are, and it will be easy to validate that our model is working correctly. The Lorenz dataset is a toy, artificial dataset generated by the latents of the Lorenz attractor. The [Lorenz attractor](https://en.wikipedia.org/wiki/Lorenz_system) is a 3-variable chaotic system that unfolds over time. The Lorenz dataset is generated by taking projections of these variables, scaling them to obtain spike rates, and generating spikes via a Poisson process. In other words:

$$
\begin{align}
\mathbf{x}_{t+1} = \text{Lorenz}(\mathbf{x}(t)) \\
\boldsymbol{\lambda}(t) &= \exp(\mathbf{C}\mathbf{x}(t) + \mathbf{d}) \\
\mathbf{y}(t) &\sim \text{Poisson}(\boldsymbol{\lambda}(t))
\end{align}
$$

The Lorenz dataset has become something of a standard for debugging models that can infer latents from observations, since it was first used to benchmark the LFADS model. Let's look at some of the data:
"""
# %%
import pickle

from pandas.core.missing import mask_missing

def load_dataset(name):
    with open(f"data/{name}_data.pkl", "rb") as f:
        return pickle.load(f)

dataset = load_dataset("lfads_lorenz")
print(dataset.keys())
# %% [markdown]
"""
These are the important keys:

* `train_data`: training data, an array of shape `(n_trials, n_timepoints, n_neurons)`. They correspond to the number of spikes (in simulated 10 ms bins) for each trial, for each neuron
* `val_data` and `val_behavior`: same, but for a validation fold. We've pre-split the data into train and validation folds. We'll train on the train fold and validate our model on the validation fold.
* `val_truth`: ground truth data for the true underlying spike rates. We know the ground truth because this is an artificial dataset.

Now let's look at the data for one trials:
"""
# %%
import matplotlib.pyplot as plt

bin_size = 0.01 # in seconds

# One figure, two axes stacked vertically
fig, axes = plt.subplots(
    nrows=2, ncols=1,           # two rows, one column
    figsize=(6, 8),             # any size you like
    sharex=False                 # optional: share the x-axis
)

axes[0].imshow(dataset['val_data'][0, :, :].T, cmap='gray_r', aspect='auto', extent=[0, dataset['val_data'].shape[1] * bin_size, 0, dataset['train_data'].shape[2]])
#axes[0].set_xlabel('Time (s)')
axes[0].set_ylabel('Neuron #')
axes[0].set_title('Spikes')

axes[1].imshow(dataset['val_truth'][0, :, :].T, cmap='gray_r', aspect='auto', extent=[0, dataset['val_data'].shape[1] * bin_size, 0, dataset['train_data'].shape[2]])
axes[1].set_xlabel('Time (s)')
axes[1].set_ylabel('Neuron #')
axes[1].set_title('Spike rates')

plt.tight_layout()
# %% [markdown]
"""
Our job will be to learn a model that can take spike data, like the one on the left, and give us back rate data, like the one on the right.
"""
# %% [markdown]
"""
Our first order of business is to create a model that can take in spike data and return (denoised) spike data: an auto-encoder.
This is going to be the scheme for our auto-encoder:

* We take one trial worth of spike data (n_timepoints, n_neurons) and embed into a series of tokens (n_tokens, latent_dim)
* We pass these tokens through a series of transformer layers. These transformer layers can peak into data from any timepoint and any neuron, and return a new set of tokens (n_tokens, latent_dim)
* At the end, we then decode back into spike data (n_timepoints, n_neurons)

We'll have done well if our autoencoder gives us back a denoised version of the input spike data.

We've covered transformers in a previous tutorial, so we won't go into too much detail here. The key idea is that transformers 
are a type of neural network that can process sequences of data, and they do so by using self-attention mechanisms to weigh 
the importance of different parts of the input sequence. Multiple transformer layers form a powerful means of processing data.

The big question is: how do we turn spike data into tokens?

We'll start with the simplest possible scheme: one token corresponds to all the spike data from a single time bin. That means 
we take the spike data, and for each time bin, we create a token that contains the spike counts for all neurons at that time bin.

In this scheme, `n_tokens` = `n_timepoints`, and `latent_dim` = `n_neurons`. Let's write out the corresponding network.
"""
# %%
import math
import torch
from torch import nn

class SimpleTransformerAutoencoder(nn.Module):
    def __init__(
        self,
        input_dim: int,
        num_layers: int = 4,
        num_heads: int = 1,
        ffn_dim: int = 256,
        dropout: float = 0.5,
        max_seq_len: int = 1000,
    ):
        super().__init__()
        self.input_dim = input_dim

        self.pos_embedding = nn.Embedding(max_seq_len, input_dim)
        self.register_buffer("pos_embedding_rg", torch.arange(max_seq_len))
        self.get_positional_encoding = lambda seq_len: self.pos_embedding(self.pos_embedding_rg[:seq_len])

        # Transformer encoder layers
        def create_encoder_layer() -> nn.TransformerEncoderLayer:
            return nn.TransformerEncoderLayer(
                d_model=self.input_dim,
                nhead=num_heads,
                dim_feedforward=ffn_dim,
                dropout=dropout,
                batch_first=True,
                norm_first=True,
            )

        self.encoder_layers = nn.ModuleList([create_encoder_layer() for _ in range(num_layers)])
        self.norm = nn.LayerNorm(input_dim)
        # This projects the output back to the input dimension
        self.output_projection = nn.Linear(input_dim, input_dim)

        # This is for regularization. We use dropout at multiple points in the network
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the transformer autoencoder.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_dim)
            
        Returns:
            Reconstructed tensor of shape (batch_size, seq_len, input_dim)
        """
        batch_size, seq_len, feature_dim = x.shape

        # Project input and add positional encoding
        x = math.sqrt(self.input_dim) * x + self.get_positional_encoding(seq_len).unsqueeze(0)
        x = self.dropout(x)
        
        # Pass through transformer encoder layers
        for layer in self.encoder_layers:
            x = layer(x)

        x = self.dropout(self.norm(x))  # Final layer normalization
        # Project back to input dimension
        x = self.output_projection(x)
        return x

n_neurons = 29  # Number of neurons in the Lorenz dataset
net = SimpleTransformerAutoencoder(n_neurons)
net

# %% [markdown]
"""
Aside from the all-important transformer encoder layers, this model has a positional encoding layer that adds positional information to the input tokens. This is important because transformers do not have any inherent notion of order, and we need to provide that information explicitly.

The way we do this is by adding a positional encoding to each token. The positional encoding is a learned embedding that encodes the position of each token in the sequence. This allows the transformer to take into account the order of the tokens when processing them.

Another important point is that we use dropout at multiple points in the network. This is a form of regularization that helps prevent overfitting. Dropout randomly sets some of the activations to zero during training, which forces the model to learn more robust features. The NDT-1 paper notes that aggressive dropout is important to make the network work well.

Our next order of business is to define a pretext task that will encourage the model to learn a good representation of the data. The pretext task we will use is a masked autoencoder. The idea is to randomly mask out some of the tokens in the input sequence, and then train the model to predict the masked tokens from the unmasked ones.

Let's see what the masking function looks like:
"""
# %%
from typing import Tuple

def do_masking(batch: torch.Tensor, mask_ratio: float = 0.25) -> Tuple[torch.Tensor, torch.Tensor]:
    """Randomly mask *mask_ratio* timesteps per trial (span width = 1)."""
    batch_size, num_timesteps = batch.shape[:2]
    mask = torch.rand(batch_size, num_timesteps) < mask_ratio
            
    # Replace some masked tokens with 0 (80%) or random spikes (20%)
    mask_token_ratio = 0.8
    random_token_ratio = 0.25

    replace_zero = (torch.rand_like(mask, dtype=float) < mask_token_ratio) & mask
    replace_rand = (torch.rand_like(mask, dtype=float) < random_token_ratio) & mask & ~replace_zero

    batch_mean = batch.to(float).mean().item()
    batch = batch.clone()  # avoid in‑place modification
    batch[replace_zero] = 0
    if replace_rand.any():
        rand_values = (torch.rand_like(batch, dtype=float) < batch_mean).to(torch.int)
        batch[replace_rand] = rand_values[replace_rand]

    return batch, mask
# %% [markdown]
"""
It's a little complicated, so let's break this down:

* The masking function takes in a batch of spike data and a mask ratio (default 0.25).
* It randomly masks out some of the tokens in the batch, with a span width of 1 (i.e., each token is masked independently).
* For the masked tokens, 80% are replaced with zeros, and 20% are those that are not replaced are replaced with random spikes.
* The unmasked tokens are untouched.

With this done, let's see what happens when we apply this masking function to fake data.
"""
# %%
import numpy as np

# Generate regular spike data from sinusoid
torch.manual_seed(48)  # For reproducible results
original_data = torch.tensor(dataset['train_data'][0:1, :, :])  # Use the first trial from the training data

# Apply masking
masked_data, mask = do_masking(original_data, mask_ratio=0.25)

# Create 4-panel visualization
fig, axes = plt.subplots(3, 1, figsize=(8, 8))

# Since we have a single sequence, we'll plot as line plots for clarity
time_axis = np.arange(original_data.shape[1])

axes[0].imshow(original_data.squeeze().numpy().T)
axes[0].set_title('Original Data (Non-masked)', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Time Steps')
axes[0].set_ylabel('Spike Value')
axes[0].grid(True, alpha=0.3)

axes[1].imshow(torch.tile(mask, [n_neurons, 1]).squeeze(), cmap='gray')
axes[1].set_title('Mask (white = masked positions)', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Time Steps')
axes[1].set_ylabel('Mask Value')
axes[1].grid(True, alpha=0.3)

axes[2].imshow(masked_data.squeeze().T)
axes[2].set_title('Masked Data', fontsize=14, fontweight='bold')
axes[2].set_xlabel('Time Steps')
axes[2].set_ylabel('Spike Value')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
# %% [markdown]
"""
Notice, in particular, how the masked data has many more zeros at early time steps. The challenge is then to reconstruct the original data from the masked data.

With that, we're ready to learn a model that can take in the masked data and predict the unmasked tokens. We're going to create our basic training loop, which should look familiar by now:

* Load the data
* Create the model
* Create the loss function
* Create the optimizer
* For each epoch:
    * For each batch:
        * Mask the data
        * Pass the masked data through the model
        * Compute the loss
        * Backpropagate and update the model parameters
    * Every few iterations:
        * Calculate the validation loss

Most of this is straightforward: the most critical piece we haven't seen is how we compute the loss. It goes like this:
"""
# %%
criterion = nn.PoissonNLLLoss(reduction="none", log_input=True)
# For one example batch, let's see how this works:
# Use the first trial from the training data
spikes = torch.tensor(dataset['train_data'][0:16, :, :]).to(torch.int)
mask_ratio = 0.25  # Masking ratio for the pretext task
spikes_masked, mask = do_masking(spikes, mask_ratio)
preds = net(spikes_masked.float())
loss = criterion(preds[mask], spikes[mask]).mean()

# %% [markdown]
"""
Notice that we:

* Only use the masked tokens to compute the loss
* Use the Poisson negative log-likelihood loss, which is appropriate for spike data

Now that we've seen each piece individually, let's train the model!
"""
# %%
import math
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import LambdaLR
from tqdm import tqdm

class WarmupCosineSchedule(LambdaLR):
    """ Linear warmup and then cosine decay.
        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.
        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.
        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.
    """
    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):
        self.warmup_steps = warmup_steps
        self.t_total = t_total
        self.cycles = cycles
        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)

    def lr_lambda(self, step):
        if step < self.warmup_steps:
            return float(step) / float(max(1.0, self.warmup_steps))
        # progress after warmup
        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))
        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))

def train_one_epoch(
    net: nn.Module,
    loader: DataLoader,
    device: torch.device,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    mask_ratio: float,
) -> float:
    net.train()
    epoch_losses = []
    for spikes, _ in loader:  # dataset returns a single tensor
        spikes = spikes.to(device)
        optimizer.zero_grad()
        spikes_masked, mask = do_masking(spikes, mask_ratio)
        preds = net(spikes_masked.float())
        loss = criterion(preds[mask], spikes[mask]).mean()
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())
    return float(np.mean(epoch_losses))

def evaluate(
    net: nn.Module,
    loader: DataLoader,
    device: torch.device,
    criterion: nn.Module,
    mask_ratio: float = 0.0,
    has_ground_truth: bool = False
) -> Tuple[float, float]:
    net.eval()
    losses, r2s = [], []
    with torch.no_grad():
        for spikes, ground_truth in loader:
            # Measure performance on the pretext task
            spikes = spikes.to(device)
            masked_spikes, mask = do_masking(spikes, mask_ratio) if mask_ratio > 0 else (spikes, torch.ones_like(spikes, dtype=torch.bool))
            preds = net(masked_spikes.float())
            loss = criterion(preds[mask], spikes[mask]).mean()
            losses.append(loss.item())

            # Measure performance on inferring latents (when that makes sense)
            if has_ground_truth:
                preds = net(spikes.float())
                preds_rates = torch.exp(preds)  # Convert from log rates to rates
            
                corr_mat = torch.corrcoef(torch.concat([preds_rates.reshape(-1, spikes.shape[2]), ground_truth.to(device).reshape(-1, spikes.shape[2])], dim=1).T)
                corrs = torch.diag(corr_mat[:corr_mat.shape[0] // 2, corr_mat.shape[0] // 2:])
                r2 = (corrs ** 2).mean().item()
                r2s.append(r2)
            else:
                r2s.append(0.0)

    return float(np.mean(losses)), float(np.mean(r2s))

def train_network(net, data, batch_size, lr, epochs, mask_ratio):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    net = net.to(device)
    train_data = torch.from_numpy(data["train_data"]).int()
    val_data = torch.from_numpy(data["val_data"]).int()

    has_ground_truth = False
    try:
        val_truth = torch.from_numpy(data["val_truth"])
        has_ground_truth = True
        print(f"Found ground truth for val data (different from input: {has_ground_truth})")
    except KeyError:
        # No ground truth available, use the same as input
        val_truth = val_data.clone()
        print("No ground truth for val data available")
    train_truth = torch.from_numpy(data.get("train_truth", train_data.numpy()))

    train_loader = DataLoader(TensorDataset(train_data, train_truth), batch_size=batch_size, shuffle=True, drop_last=False)
    val_loader = DataLoader(TensorDataset(val_data, val_truth), batch_size=batch_size, shuffle=False)

    criterion = nn.PoissonNLLLoss(reduction="none", log_input=True)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)

    scheduler = WarmupCosineSchedule(
        optimizer,
        warmup_steps=int(epochs * 0.1),  # 10% of total epochs as warmup
        t_total=epochs,
    )

    best_val_loss = float("inf")

    for epoch in range(1, epochs):
        
        train_loss = train_one_epoch(
            net, train_loader, device, criterion, optimizer, mask_ratio=mask_ratio
        )
        val_loss, val_r2 = evaluate(
            net, val_loader, device, criterion, 
            mask_ratio=mask_ratio, 
            has_ground_truth=has_ground_truth
        )

        tqdm.write(
            f"Epoch {epoch:03d} | train NLL {train_loss:.4f} | val NLL {val_loss:.4f} | val R² {val_r2:.4f}"
        )

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = {k: v.cpu().detach().clone() for k, v in net.state_dict().items()}
            
        scheduler.step()

    # Load the best model
    net.load_state_dict(best_model)
    return net


net = SimpleTransformerAutoencoder(
    input_dim=n_neurons,
    num_layers=6,  
    num_heads=1, 
    ffn_dim=64, 
    dropout=0.7, 
    max_seq_len=50, 
)
batch_size = 64
lr = 2e-3  # Learning rate
epochs = 100  # Number of epochs to train
mask_ratio = 0.25

# Train the network
train_network(
    net,
    dataset,
    batch_size=batch_size,
    lr=lr,
    epochs=epochs,
    mask_ratio=mask_ratio
)

# %% [markdown]
"""
The training converged! We can see that the validation loss is decreasing, and the validation R² is increasing.

Let's visualize the results on some sample data. We'll take a single validation trial and see how the network embeds the data. You'll note that we don't mask the data in this case: we just take the raw data and look at how the auto-encoder treats it.
"""
# %%
# Which trial to visualize?
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
visualize_index = 2
net.eval()
example_batch = torch.tensor(dataset['val_data'][visualize_index:visualize_index+1, :, :]).int().to(device)
true_rates = torch.tensor(dataset['val_truth'][visualize_index:visualize_index+1, :, :]).to(device)

estimated_spike_log_rates = net(example_batch.float())
estimated_spike_rates = torch.exp(estimated_spike_log_rates).detach().cpu().numpy()[0]

plt.figure(figsize=(4, 12))
plt.subplot(3, 1, 1)
plt.imshow(example_batch[0].detach().cpu().numpy().T, aspect="auto", origin="lower")
plt.title("Input spikes")
plt.subplot(3, 1, 2)
plt.imshow(true_rates[0].detach().cpu().numpy().T, aspect="auto", origin="lower")
plt.title("Ground truth latents")
plt.subplot(3, 1, 3)
plt.imshow(estimated_spike_rates.T, aspect="auto", origin="lower")
plt.title("Model prediction")
plt.show()

# %% [markdown]
"""
Either fix this or justify why this works very poorly. Now add the embedding linear layer:
"""
# %%
class TransformerAutoencoder(nn.Module):
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 256,
        num_layers: int = 4,
        num_heads: int = 1,
        ffn_dim: int = 256,
        dropout: float = 0.5,
        max_seq_len: int = 1000,
    ):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        self.pos_embedding = nn.Embedding(max_seq_len, hidden_dim)
        self.register_buffer("pos_embedding_rg", torch.arange(max_seq_len))
        self.get_positional_encoding = lambda seq_len: self.pos_embedding(self.pos_embedding_rg[:seq_len])

        # Transformer encoder layers
        def create_encoder_layer() -> nn.TransformerEncoderLayer:
            return nn.TransformerEncoderLayer(
                d_model=self.hidden_dim,
                nhead=num_heads,
                dim_feedforward=ffn_dim,
                dropout=dropout,
                batch_first=True,
                norm_first=True,
            )

        self.input_projection = nn.Linear(input_dim, hidden_dim, bias=False)
        self.encoder_layers = nn.ModuleList([create_encoder_layer() for _ in range(num_layers)])
        self.norm = nn.LayerNorm(hidden_dim)
        # This projects the output back to the input dimension
        self.output_projection = nn.Linear(hidden_dim, input_dim)

        # This is for regularization. We use dropout at multiple points in the network
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, return_latents=False):
        """
        Forward pass of the transformer autoencoder.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_dim)
            
        Returns:
            Reconstructed tensor of shape (batch_size, seq_len, input_dim)
        """
        batch_size, seq_len, feature_dim = x.shape

        # Project input and add positional encoding
        x = self.dropout(x)  
        x = math.sqrt(self.input_dim) * self.input_projection(x) + self.get_positional_encoding(seq_len).unsqueeze(0)
        x = self.dropout(x)
        
        # Pass through transformer encoder layers
        for layer in self.encoder_layers:
            x = layer(x)

        x = self.norm(x) # Final layer normalization
        if return_latents: 
            x_latents = x.clone()  # Save latents if requested

        x = self.dropout(x)
        # Project back to input dimension
        x = self.output_projection(x)
        if return_latents:
            return x, x_latents
        else:
            return x

net = TransformerAutoencoder(
    input_dim=n_neurons,
    hidden_dim=256,
    num_layers=4,  
    num_heads=1, 
    ffn_dim=128, 
    dropout=0.7, 
    max_seq_len=50, 
)
batch_size = 64
lr = 2e-3  # Learning rate
epochs = 100  # Number of epochs to train
mask_ratio = 0.25

# Train the network
train_network(
    net,
    dataset,
    batch_size=batch_size,
    lr=lr,
    epochs=epochs,
    mask_ratio=mask_ratio
)

# %%
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
visualize_index = 2
net.eval()
example_batch = torch.tensor(dataset['val_data'][visualize_index:visualize_index+1, :, :]).int().to(device)
true_rates = torch.tensor(dataset['val_truth'][visualize_index:visualize_index+1, :, :]).to(device)

estimated_spike_log_rates = net(example_batch.float())
estimated_spike_rates = torch.exp(estimated_spike_log_rates).detach().cpu().numpy()[0]

plt.figure(figsize=(4, 12))
plt.subplot(4, 1, 1)
plt.imshow(example_batch[0].detach().cpu().numpy().T, aspect="auto", origin="lower")
plt.title("Input spikes")
plt.subplot(4, 1, 2)
plt.imshow(true_rates[0].detach().cpu().numpy().T, aspect="auto", origin="lower")
plt.title("Ground truth latents")
plt.subplot(4, 1, 3)
plt.imshow(estimated_spike_rates.T, aspect="auto", origin="lower")
plt.title("Model prediction")
plt.subplot(4, 1, 4)
plt.plot([0, 6], [0, 7], 'k-')
plt.plot(estimated_spike_rates.T.ravel(), true_rates[0].detach().cpu().numpy().T.ravel(), '.')
plt.xlabel("Estimated spike rates")
plt.ylabel("True spike rates")
plt.show()

# %% [markdown]
"""
That looks even better. An advantage of doing the autoencoding in latent space is that we can flexibly change the latent dimensionality of the network, increasing the capacity of the model without affecting the number of layers. 

Does the latent space encode something interesting about the dynamical system that generated this data? Transformers have a reputation as black boxes, but nothing prevents us from looking at what's inside the models to learn about how they operate. We can verify this by looking at the weights of the model. Let's use PCA to determine the measure the top PCs of the input embedding matrix and the output readout matrix.
"""
# %%
Wi = net.input_projection.weight.detach().cpu().numpy()
Wo = net.output_projection.weight.detach().cpu().numpy()

Ui, Si, Vi = np.linalg.svd(Wi, full_matrices=False)
Uo, So, Vo = np.linalg.svd(Wo, full_matrices=False)

plt.figure(figsize=(8, 8))
plt.subplot(2, 2, 1)
plt.plot(Si, 'o-', label='Input embedding singular values')
plt.subplot(2, 2, 2)
plt.plot(So, 'o-', label='Output readout singular values')
plt.subplot(2, 2, 3)
plt.imshow(Ui[:, :3], aspect='auto', origin='lower')
plt.title('Top 3 input projection singular vectors')
plt.subplot(2, 2, 3)
plt.imshow(Uo[:, :3], aspect='auto', origin='lower')
plt.title('Top 3 output projection singular vectors')

# %% [markdown]
"""
Notice that the singular values of the input and output embedding matrices fall off dramatically after the 3rd singular value. Interestingly, the Lorenz dynamical system is 3-dimensional, so this suggests that the model has learned to embed the data in a 3-dimensional latent space.

Notice also the structure of the input and output projection singular vectors. They are highly structured, and indeed seem to distinguish the first half of the neurons from the second half. You will notice that this reflects the structure of the observations, where the loadings of the first half of the neurons are different than those of the second half.

Thus, it appears that one of the things the model has learned is that the data lies on a low-rank manifold, a core assumption of many models that learn structure from neural data, including PCA, LFADS, and GPFA.

# Real data: `mc_maze`

Now that we've gotten a handle on working with toy data, let's switch over to real data. A typical experiment where you might want to apply a foundation model for neuroscience is a BCI decoding experiment. In the `mc_maze` series of datasets (`mc_maze`, `mc_maze_large`, `mc_maze_medium`, `mc_maze_small`), a monkey completes a reaching task where he needs to trace with his finger on a touchscreen from a start position to an end position, avoiding the maze walls. Neurons are recorded in premotor cortex (PMd) and in M1. 

The data is similarly structured to the Lorenz dataset, with a few key differences:

* `val_truth`: "Ground truth" data for the true underlying spike rates. Unlike the Lorenz dataset, we can never truly know what this is. Instead, we estimate it from taking an average over similar trials and applying a 50 ms smoothing window.
* `train_behavior` and `val_behavior`: Aligned behavior, an array of shape `(n_trials, 2)`. This corresponds to the monkey's arm velocity, in `m/s`. We can use this to train a BCI decoder.

Let's start by visualizing this data.
"""
# %%
import matplotlib.pyplot as plot

dataset = load_dataset("mc_maze_large")

# One figure, two axes stacked vertically
fig, (ax_top, ax_bottom) = plt.subplots(
    nrows=2, ncols=2,           # two rows, one column
    gridspec_kw={'height_ratios': [1, 2], 'width_ratios': [2, 1]},  # 1 : 2  ⇒ top = ⅓, bottom = ⅔
    figsize=(6, 8),             # any size you like
    sharex=False                 # optional: share the x-axis
)

bin_size = .01

ax_top[0].plot(np.arange(dataset['val_behavior'].shape[1]) * bin_size, dataset['val_behavior'][0, :])
ax_top[0].legend(['Velocity (x)', 'Velocity (y)'])
ax_top[0].set_xlim(0, dataset['train_behavior'].shape[1] * bin_size)
ax_top[0].set_ylim(-600, 600)

ax_top[1].plot(np.cumsum(dataset['val_behavior'][0, :, 0]), np.cumsum(dataset['val_behavior'][0, :, 1]), '-.')
ax_top[1].set_xlabel('position (x)')
ax_top[1].set_ylabel('position (y)')
ax_top[1].set_xlim([-10000, 10000])
ax_top[1].set_ylim([-10000, 10000])
ax_top[1].plot(np.cumsum(dataset['val_behavior'][0, :, 0])[-1], np.cumsum(dataset['val_behavior'][0, :, 1])[-1], 'gx')  # mark the end
ax_top[1].plot(0, 0, 'ro')

ax_bottom[0].imshow(dataset['val_data'][0, :, :].T, cmap='gray_r', aspect='auto', extent=[0, dataset['val_data'].shape[1] * bin_size, 0, dataset['val_data'].shape[2]])
ax_bottom[0].set_xlabel('Time (s)')
ax_bottom[0].set_ylabel('Neuron #')
ax_bottom[0].set_title('Spikes')

ax_bottom[1].imshow(dataset['val_truth'][0, :, :].T, cmap='gray_r', aspect='auto', extent=[0, dataset['val_truth'].shape[1] * bin_size, 0, dataset['val_truth'].shape[2]])
ax_bottom[1].set_xlabel('Time (s)')
ax_bottom[1].set_ylabel('Neuron #')
ax_bottom[1].set_title('Ground truth (smoothed data)')

# %% [markdown]
"""
We can proceed as we did previously to train a masked autoencoder on this data. 
"""
# %%
net = TransformerAutoencoder(
    input_dim=dataset['train_data'].shape[2],
    hidden_dim=256,
    num_layers=4,  
    num_heads=1, 
    ffn_dim=128, 
    dropout=0.7, 
    max_seq_len=50, 
)
batch_size = 64
lr = 2e-3  # Learning rate
epochs = 100  # Number of epochs to train
mask_ratio = 0.25

# Train the network
train_network(
    net,
    dataset,
    batch_size=batch_size,
    lr=lr,
    epochs=epochs,
    mask_ratio=mask_ratio
)

# %% [markdown]
"""
That worked! Now we have a good representation of the data, but we are not yet able to decode the behavior, for example for a brain computer interface.

So how could we use this BCI decoding with transformers? Well, we could:

* Supervised learning: Train a transformer from scratch end-to-end to predict the desired behavior
* Supervised learning + auxillary loss: Train a transformer to both reconstruct the input AND predict the desired behavior
* Transfer learning: First train a transformer on a masked autoencoding task, then fine-tune it on the BCI decoding task

In this tutorial, we'll focus on the final approach. Our TransformerAutoencoder implementation already has a `return_latents` argument that allows us to return the latents from the forward pass. We can use this to train a BCI decoder on top of the latents.

Let's create a lightweight shim on top of the TransformerAutoencoder that allows us to train a BCI decoder on top of the latents. We'll use the simplest setup, where the sampling rate of the behavior is the same as the sampling rate of the spikes, and spikes and behavior are already aligned temporally. In that case, we can just use a lightweight linear layer to decode the behavior from the latents: one token = one timepoint = one behavioral sample. Note that you could use a more powerful decoder like another transformer, or shift things so you have a different sampling rate than the spikes---see the references for details.
"""
# %% [markdown]
class TransformerWithDecoder(nn.Module):
    """Combines pretrained PM Transformer with behavior decoder."""
    
    def __init__(
        self,
        transformer: nn.Module,
        behavior_dim: int,
        freeze_transformer: bool = True,
        passthrough: bool = False,
        target_layer: int = -1,
        input_dim: int = None,
    ):
        super().__init__()
        self.passthrough = passthrough
        self.transformer = transformer
        self.target_layer = target_layer

        hidden_dim = transformer.hidden_dim
            
        self.decoder = nn.Linear(
            in_features=hidden_dim,
            out_features=behavior_dim
        )
        
        self.set_freeze_transformer(freeze_transformer)
                
    def set_freeze_transformer(self, freeze: bool):
        """Freeze or unfreeze transformer parameters."""
        for param in self.transformer.parameters():
            param.requires_grad = not freeze
            
        # DO NOT FREEZE THE INPUT LAYER
        self.transformer.input_projection.weight.requires_grad = True
                
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: Input spikes (B, T, neurons)
        Returns:
            Tuple of (reconstructed spikes, decoded behavior)
        """
        # Get internal representations
        _, h = net.forward(x=x, return_latents=True)
            
        # Decode behavior from representations
        behavior = self.decoder(h)
        
        return behavior

# %% [markdown]
"""
Now we're ready to train this. We set up another training loop. Note that this time, our criterion will be the MSE loss, since we're predicting continuous behavior values. We also use far more conservative dropout rate.
"""
# %%
def compute_r2(pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:
    """Compute R² score."""
    # Flatten batch and time dimensions
    pred_flat = pred.reshape(-1, pred.shape[-1])
    true_flat = true.reshape(-1, true.shape[-1])
    
    # Compute R² for each behavior dimension
    r2_per_dim = []
    for i in range(pred.shape[-1]):
        # Compute correlation coefficient
        if pred_flat[:, i].std() > 0 and true_flat[:, i].std() > 0:
            corr = torch.corrcoef(
                torch.stack([pred_flat[:, i], true_flat[:, i]])
            )[0, 1]
            r2_per_dim.append(corr ** 2)
        else:
            r2_per_dim.append(torch.tensor(0.0))
    
    return torch.stack(r2_per_dim).mean()

def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    device: torch.device,
    optimizer: torch.optim.Optimizer,
) -> Tuple[float, float]:
    """Train for one epoch."""
    model.train()
    epoch_losses = []
    epoch_r2s = []
    
    criterion_mse = nn.MSELoss()
    
    for spikes, _, behavior in loader:
        spikes = spikes.to(device).float()
        behavior = behavior.to(device).float()
        
        optimizer.zero_grad()
        
        # Forward pass
        _, pred_behavior = model(spikes)
        
        # Compute loss
        loss = criterion_mse(pred_behavior, behavior)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Track metrics
        epoch_losses.append(loss.item())
        with torch.no_grad():
            r2 = compute_r2(pred_behavior, behavior)
            epoch_r2s.append(r2.item())
    
    return float(np.mean(epoch_losses)), float(np.mean(epoch_r2s))


def evaluate(
    model: nn.Module,
    loader: DataLoader,
    device: torch.device,
) -> Tuple[float, float]:
    """Evaluate model."""
    model.eval()
    losses = []
    r2s = []
    
    criterion_mse = nn.MSELoss()
    
    with torch.no_grad():
        for spikes, _, behavior in loader:
            spikes = spikes.to(device).float()
            behavior = behavior.to(device).float()
            
            # Forward pass
            _, pred_behavior = model(spikes)
            
            # Compute metrics
            loss = criterion_mse(pred_behavior, behavior)
            r2 = compute_r2(pred_behavior, behavior)
            
            losses.append(loss.item())
            r2s.append(r2.item())
    
    return float(np.mean(losses)), float(np.mean(r2s))

def train_bci_decoder(
        model: nn.Module,
        dataset,
        batch_size,
        lr,
        epochs,
    ):
    train_loader = DataLoader(
        TensorDataset(
            torch.from_numpy(dataset['train_data']).int(),
            torch.from_numpy(dataset['train_behavior']).float(),
        ),
        batch_size=batch_size,
        shuffle=True,
        drop_last=False
    )
    val_loader = DataLoader(
        TensorDataset(
            torch.from_numpy(dataset['val_data']).int(),
            torch.from_numpy(dataset['val_behavior']).float(),
        ),
        batch_size=batch_size,
        shuffle=False,
        drop_last=False
    )

    # Create new optimizer with all parameters
    optimizer = torch.optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr * 0.1  # Use lower learning rate for fine-tuning
    )
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)
    for epoch in range(1, \epochs + 1):
        # Switch to end-to-end training after decoder-only epochs        
        train_loss, train_r2 = train_one_epoch(
            model, train_loader, device, optimizer
        )
        val_loss, val_r2 = evaluate(model, val_loader, device)
        tqdm.write(
            f"Epoch {epoch:03d} | "
            f"train loss {train_loss:.4f} | train R² {train_r2:.4f} | "
            f"val loss {val_loss:.4f} | val R² {val_r2:.4f}"
        )
        scheduler.step(val_r2)
    return model


# %% [markdown]
"""
Now we're ready to train the model. We'll use two different training modes:
* Frozen encoder: We freeze the transformer encoder and only train the decoder. This is useful for transfer learning, where we want to leverage the pretrained representations.
* End-to-end training: We unfreeze the transformer encoder and train the entire model end-to-end. This is useful for fine-tuning the model on the specific task.
"""

# %%
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
for frozen_encoder in [True, False]:
    # Create the model with frozen transformer
    print(f"Training with {'frozen' if frozen_encoder else 'unfrozen'} transformer encoder")
    model = TransformerWithDecoder(
        transformer=net,
        behavior_dim=dataset['train_behavior'].shape[1],
        freeze_transformer=frozen_encoder,  # Freeze the transformer encoder
        passthrough=False,  # Don't pass through the input spikes
    )
    model = model.to(device)

    batch_size = 64
    lr = 1e-3  # Learning rate
    epochs = 50  # Number of epochs to train
    # Train the model with frozen transformer
    model = train_bci_decoder(
        model,
        dataset,
        batch_size=batch_size,
        lr=lr,
        epochs=epochs,
    )

# %% [markdown]
"""
Great! We see that the performance of the BCI decoder is quite a bit better when the model is trained end-to-end than when it is trained with a frozen encoder. This is expected, as the model can adapt the representations to the specific task.

But how well does the model perform on the BCI decoding task? We have to compare our model against baselines!

Let's make up two baselines:

* Linear decoder: we'll have a simple linear decoder on top of smoothed spikes.
* Transformer decoder trained from scratch: we'll train a transformer from scratch on the BCI decoding task.

Then we'll have a clear baseline to compare against.
"""
# %%
def gaussian_smooth_1d(x, sigma=5):
    """
    Apply Gaussian smoothing along the time dimension.
    
    Args:
        x: Input tensor of shape (batch, time, neurons)
        sigma: Standard deviation of Gaussian kernel
    
    Returns:
        Smoothed tensor of same shape as input
    """
    batch, time, neurons = x.shape
    
    # Create Gaussian kernel
    # Kernel size should be odd and large enough to capture the Gaussian
    kernel_size = int(6 * sigma + 1)  # 6 sigma captures 99.7% of distribution
    if kernel_size % 2 == 0:
        kernel_size += 1  # Ensure odd size
    
    # Create 1D Gaussian kernel
    kernel = torch.arange(kernel_size, dtype=torch.float32)
    kernel = kernel - kernel_size // 2  # Center around 0
    kernel = torch.exp(-0.5 * (kernel / sigma) ** 2)
    kernel = kernel / kernel.sum()  # Normalize
    
    # Move kernel to same device as input
    kernel = kernel.to(x.device)
    
    # Reshape for conv1d: (batch, time, neurons) -> (batch * neurons, 1, time)
    x_reshaped = x.permute(0, 2, 1).reshape(batch * neurons, 1, time)
    
    # Reshape kernel for conv1d: needs shape (out_channels, in_channels, kernel_size)
    kernel = kernel.view(1, 1, -1)
    
    # Apply convolution with padding to maintain time dimension
    padding = kernel_size // 2
    x_smoothed = F.conv1d(x_reshaped, kernel, padding=padding)
    
    # Reshape back: (batch * neurons, 1, time) -> (batch, time, neurons)
    x_smoothed = x_smoothed.view(batch, neurons, time).permute(0, 2, 1)
    
    return x_smoothed

class SmoothDecoder(nn.Module):
    def __init__(self, input_dim, output_dim, sigma):

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.sigma = sigma

        self.output_decoder = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        """
        Forward pass of the smooth transformer.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, input_dim)
            
        Returns:
            Reconstructed tensor of shape (batch_size, seq_len, input_dim)
        """
        # Apply Gaussian smoothing
        x_smoothed = gaussian_smooth_1d(x, self.sigma)
        
        # Pass through the transformer
        return self.output_decoder(x_smoothed)

# Train the model with a smooth decoder

batch_size = 64
lr = 1e-3  # Learning rate
epochs = 50  # Number of epochs to train
# Train the model with frozen transformer
model = SmoothDecoder(
    input_dim=dataset['train_data'].shape[2],
    output_dim=dataset['train_behavior'].shape[1],
    sigma=5,  # Standard deviation for Gaussian smoothing
)

model = train_bci_decoder(
    model,
    dataset,
    batch_size=batch_size,
    lr=lr,
    epochs=epochs,
)

net = TransformerAutoencoder(
    input_dim=dataset['train_data'].shape[2],
    hidden_dim=256,
    num_layers=4,
    num_heads=1,
    ffn_dim=128,
    dropout=0.7
)

# Train with a transformer decoder from scratch
model = TransformerWithDecoder(transformer=net)
model = model.to(device)

model = train_bci_decoder(
    model,
    dataset,
    batch_size=batch_size,
    lr=lr,
    epochs=epochs,
)

# %% [markdown]
"""
Great! **Write some hard-earned lessons here**.

# Transfer learning from a larger model

In practice, what we typically do with foundation models for neuroscience is to train a large model on a large dataset, and then finetune on a smaller dataset. We'll explore this setup here. 

What we've done is train a model offline on the full `mc_maze` dataset. This is still fairly small by foundation model standards, but it contains thousands of trials from which to learn a good representation. In the real world, you would typically train on much larger datasets like you might find on DANDI. But this is enough to demonstrate the core ideas.

Now we'll load up the weights and transfer it to the `mc_maze_medium` dataset. There's one very important thing we'll need to take care of, however:

## Changing the input dimensionality

Although the `mc_maze` series of datasets were all collected in the animal with the same implants, there are different numbers of neurons recorded in each dataset. Thus, we can't use the entire model weights: we'll need to carefully swap the input and output projection layers to match the new number of neurons.

Then we'll go ahead and train the model and see how well it performs on the new dataset.
"""
# %%
# Load the pretrained model
net = TransformerAutoencoder(
    input_dim=dataset['train_data'].shape[2],
    hidden_dim=256,
    num_layers=6,
    num_heads=2,
    ffn_dim=128,
    dropout=0.1,
    max_seq_len=50,
)

# Here's the tricky part: we'll overwrite the input and output projection layers to match the new number of neurons
pretrained_model_path = "path/to/pretrained_model.pth"  # Adjust this path
ckpt = torch.load("pretrained_model_path", map_location=device)
net.load_state_dict(ckpt, strict=False)

# %% [markdown]
"""
Notice that this returned an error, because the input and output projection layers don't match the new number of neurons. We'll need to overwrite them.
"""
# %%
# The input and output projection layers need to match the new number of neurons, so we'll just use the weights that are already in the model.
ckpt['input_projection'] = net.input_projection.weight.data.detach().cpu().numpy()
ckpt['output_projection'] = net.input_projection.weight.data.detach().cpu().numpy()
net.load_state_dict(ckpt, strict=False)

# %%
"""
Now we're ready to train the model on the new dataset. We'll use the same training loop as before, but this time we'll train the entire model end-to-end.
"""
# %%
model = train_bci_decoder(
    model=TransformerWithDecoder(
        transformer=net,
        behavior_dim=dataset['train_behavior'].shape[1],
        freeze_transformer=False,  # Unfreeze the transformer encoder
    ),
    dataset=dataset,
    batch_size=batch_size,
    lr=lr,
    epochs=epochs,
)

# %% [markdown]
"""
And there we have it: a model, trained from scratch on a large dataset, that can be adapted to a smaller dataset with a few lines of code. This is the power of foundation models for neuroscience: they allow us to leverage large datasets and transfer learning to build powerful models that can be adapted to new tasks with minimal effort.

Let's visualize all the scores of the different model variants we've tried on this dataset.
"""
# %%
# TODO: show the scores
def plot_scores(scores):
    """Plot scores for different model variants."""
    fig, ax = plt.subplots(figsize=(8, 6))
    for label, score in scores.items():
        ax.plot(score['epoch'], score['r2'], label=label)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('R² Score')
    ax.legend()
    plt.show()

# %% [markdown]
"""
# Causal decoding

We just one more concept we'll need to cover: causal decoding. Thus far, we've used all spikes to predict behavior. But if we wanted to use this for online BCI decoding, we could only use spikes that have happened thus far.

The fix is to use a causal transformer decoder. To prevent information from flowing from the future to the point, we'll use a mask that tells the model to only use the first token to reconstruct the first token; the first two tokens to reconstruct the second token; the first three tokens to reconstruct the third token, and so on. This is a causal decoder, and it allows us to use the model for online BCI decoding.

This mask, which is a lower triangular matrix, can be created with the `torch.tril` function. We'll use this to create a causal transformer decoder that can be used for online BCI decoding.
"""
# %%
mask = torch.tril(torch.ones((50, 50), dtype=torch.bool))
plt.imshow(mask, cmap='gray', aspect='auto')
plt.xlabel('Time (bins) – information flows from')
plt.ylabel('Time (bins) - information flows to')
plt.title("Example causal mask")

# %% [markdown]
"""
Now let's implement the causal transformer decoder. We'll use the same architecture as before, but we'll add a causal mask to the transformer layers. This will ensure that the model only uses information from the past to predict the future.
"""
# %%

# %%
"""
Now we've successfully implemented a causal transformer decoder. This allows us to use the model for online BCI decoding, where we can only use spikes that have happened thus far to predict behavior.

# Conclusion

In this tutorial, we've covered the following topics:

* How to implement a masked autoencoder using transformers
* How to train a transformer autoencoder on a toy dataset
* How to apply the transformer autoencoder to real data
* How to train a BCI decoder on top of the transformer latents
* How to adapt a pretrained transformer model to a new dataset
* How to implement a causal transformer decoder for online BCI decoding

So how can you take this to the next level? The literature is rife with ideas for how to improve this model. Here are a few suggestions:

* Use a more sophisticated tokenization scheme, such as patching, spike-based tokenization, or latent space tokenization
* Use a more sophisticated pretext task, such as next-token prediction, or contrastive learning
* Use a more sophisticated decoder, such as another transformer or a recurrent neural network
* Use a bigger model on a bigger dataset

When you're ready to take that leap, check out the tutorial from Eva Dyer's group on [Foundation Models for Neuroscience](https://colab.research.google.com/github/evadyer/foundation_models_for_neuroscience/blob/main/01_foundation_models_for_neuroscience.ipynb). It covers many of these ideas in more detail, and provides a great starting point for your own research.
Here are a few other relevant references:

* 
* 
* 
"""
# %%
"""
* Check that we can recover the latents
* Check what's in the weight matrices

Now let's see how we can apply this kind of model to real data.

* Use on `mc_maze_small` dataset
* Now bring in `mc_maze` and show how we can adapt this model to other datasets
"""

# %% [markdown]
"""
1. Our goal is to find a good representation of this data. We’ll create a masked autoencoder. We’ll use transformers. Transformers work on tokens, so we need to decide on a tokenization scheme. NDT-1 makes a simple choice: one time bin, all neurons = one token. Note that there are many other tokenization schemes:
    1. one token = multiple time bins, multiple neurons (patching)
    2. one token = all time bins, one neuron
    3. one token = one spike (the POYO choice)
2. Next, we need a pretext task to encourage our model to learn a good representation of the data. Masked autoencoding is such a task. 
    1. Split tokens into two: masked (25%) and unmasked (75%)
    2. For the masked tokens, 80% will be replaced with zeros, 5% will be replaced by noise, the rest will be left alone
    3. The unmasked tokens are untouched
    4. The task: predict unmasked tokens from all (masked + unmasked tokens)
    5. (demonstrate this in a cartoon fashion)
3. With that, we have our core ingredients. Let’s build a simple transformer that takes in the raw data and predicts the left-out token
    1. The latent dimensionality is the same as the dimensionality of the data
    2. We add some positional encodings over time
    3. We add an N-layer transformer (we use N=2)
    4. At the end of the line, we have an output nonlinearity
    5. We add a penalty for the log-likehood (Poisson loss)
    6. We train the model (train within the colab)
4. Let’s verify this finds some reasonable latents (look at the true latents and see whether they align with the inferred ones)
5 (bonus). What do we know about the role of different position embeddings? Watch what happens if we remove the sinusoid embeddings
6 (bonus). Consider a next-token prediction task instead


To make sure we’re doing well, we’ll use another dataset where we know what the actual latents are. 


## The datasets and the task

Let’s visualize what we’re working with. A typical experiment where you might want to apply a foundation model for neuroscience is a BCI decoding experiment. In the `mc_maze` series of datasets (`mc_maze`, `mc_maze_large`, `mc_maze_medium`, `mc_maze_small`), a monkey completes a reaching task where he needs to trace with his finger on a touchscreen from a start position to an end position, avoiding the maze walls. Neurons are recorded in premotor cortex (PMd) and in M1. 

I've created pickle files from the raw data, which is stored in the DANDI archive. The purpose of these pickle files is to make it easy for you to read this raw data and train a model from it. Let's see the different keys in the pickle file:

```
import pickle

with open("mc_maze_data.pkl", "rb") as f:
    data = pickle.load(f)

pickle.keys()
```

These are the important keys:

```
* `train_data`: training data, an array of shape `(n_trials, n_timepoints, n_neurons)`. They correspond to the number of spikes (in 10 ms bins) for each trial, for each neuron
* `train_behavior`: Aligned behavior, an array of shape `(n_trials, 2)`. This corresponds to the monkey's arm velocity, in `m/s`
* `val_data` and `val_behavior`: same, but for a validation fold. We've pre-split the data into train and validation folds. We'll train on the train fold and validate our model on the validation fold.
* `val_ground_truth`: "Ground truth" data for the true underlying spike rates. This was generated by taking an average over similar trials and applying a 50 ms smoothing window. 

Let’s see some raster plots from one example trial. 

[code]

Notice that the data is quite noisy. There are many things we can do with this dataset, including:

- Recovering the underlying spike rates of the data from spikes. Although data 
- Predicting the monkey’s arm position. Perhaps we’d like to train a BCI. That would involve decoding the monkey’s hand position from just his brain activity. We could train based on the monkey’s real movement, and then apply the decoder so the monkey can simply imagine the movement instead.


## Section 1: recovering the latents via a single-session autoencoder

[show the image of NDT-1 here]

1. Our goal is to find a good representation of this data. We’ll create a masked autoencoder. We’ll use transformers. Transformers work on tokens, so we need to decide on a tokenization scheme. NDT-1 makes a simple choice: one time bin, all neurons = one token. Note that there are many other tokenization schemes:
    1. one token = multiple time bins, multiple neurons (patching)
    2. one token = all time bins, one neuron
    3. one token = one spike (the POYO choice)
2. Next, we need a pretext task to encourage our model to learn a good representation of the data. Masked autoencoding is such a task. 
    1. Split tokens into two: masked (25%) and unmasked (75%)
    2. For the masked tokens, 80% will be replaced with zeros, 5% will be replaced by noise, the rest will be left alone
    3. The unmasked tokens are untouched
    4. The task: predict unmasked tokens from all (masked + unmasked tokens)
    5. (demonstrate this in a cartoon fashion)
3. With that, we have our core ingredients. Let’s build a simple transformer that takes in the raw data and predicts the left-out token
    1. The latent dimensionality is the same as the dimensionality of the data
    2. We add some positional encodings over time
    3. We add an N-layer transformer (we use N=2)
    4. At the end of the line, we have an output nonlinearity
    5. We add a penalty for the log-likehood (Poisson loss)
    6. We train the model (train within the colab)
4. Let’s verify this finds some reasonable latents (look at the true latents and see whether they align with the inferred ones)
5 (bonus). What do we know about the role of different position embeddings? Watch what happens if we remove the sinusoid embeddings
6 (bonus). Consider a next-token prediction task instead

## Section 2: an autoencoder that can switch different recordings

Thus far, we’ve trained an auto-encoder from scratch. However, it is very specific to this recording session: what if we had recordings from two different monkeys doing the same task? Then we’d need to train two separate autoencoders. There is no natural link between the units for monkey 1 and for monkey 2.

In this section, we’ll repeat the same exercise but *in latent space*. We use a scheme inspired by NDT-1-stitch, which is also similar to the one used by many others, including POYO: we use a linear projection to perform the analysis in latent space. The latent space does alignment.

<show image of NDT-1 stitch>

- Add the linear encoder and decoder layers
- Apply on the Lorenz dataset–what do the embeddings look like? Look at the first two PCs of the embeddings and see what they look like
- Apply on the mc-maze small dataset → can we read off the identity of the relevant areas?
- Apply on mc-maze small and medium datasets → do the embeddings look similar? What would happen if swap in one embedding for the other?

## Section 3: using the model

So far, we’ve used a pretext task to learn a good representation of neural data. But how can we use this model to perform a downstream task? Let’s use one downstream task, decoding the hand velocity of a monkey during a task. This could be the basis for a brain-computer interface, where we replace the monkey’s real movement with an intended movement of a cursor.

- Frozen model, linear decoder: let’s put a linear decoder on top of the model. We strip out the top of the model that transforms the latents into spike rates, and instead put a linear regression on top of it. There’s necessarily a lag between the monkey’s brain activity and the movement of the arm.
    - Using a grid search on the latents of the model, let’s find the best lag. What is it? Let’s use this lag from now on.
    - Let’s evaluate the model in a leave-one-out environment. How well does this work?
- Frozen model, causal linear decoder. One issue here is that our model is acausal: although the linear decoder uses only past data, the transformer mixes tokens from the past *and* the future. That means we could not use the model as is as an online decoder. Let’s use attention masking to ensure that the model only uses past spike data. How well does that work?
- Causal linear decoder, end-to-end training. It’s not clear that the pretrained model will work well, because the pretext task could leverage the past and future. Can we do better if we train the model end-to-end?
- Did we actually do anything? Train a very simple causal lagged linear regression model end-to-end. How well does that work?

## Section 4: scaling up

We’ve trained our model on a tiny amount of data. We could scale up to vastly larger amounts of data with a significant amount of engineering. A big question in building foundation models for neuroscience is dealing with the nuts and bolts of data engineering: neural data can be in trials, continuous, readouts can be discrete, continuous, etc. And that means that to scale up to thousands of hours of data, we need to work on good data pipelines. See this tutorial from Eva Dyer’s group from Cosyne here:

- https://cosyne-tutorial-2025.github.io/
"""